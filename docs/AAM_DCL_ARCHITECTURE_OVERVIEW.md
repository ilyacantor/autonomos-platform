# AAM and DCL Architecture Overview
## System Architecture for Enterprise-Scale Data Integration

**Version:** 3.0 PRODUCTION STATE UPDATE  
**Original Date:** November 17, 2025  
**Last Updated:** November 19, 2025  
**Status:** Foundation Complete, Production Infrastructure Operational  
**Focus:** Actual implementation state, validated performance, enterprise readiness

---

# ðŸŽ¯ CURRENT PRODUCTION STATE (November 18, 2025)

**Status**: **Phase 1 Foundation Complete + Enterprise Infrastructure Operational**  
**Validation**: 25+ architect reviews, 178 tests (93.4% pass rate), zero startup errors

## What's Actually Built and Operational

### âœ… **AAM (Adaptive API Mesh) - Production Ready**

#### Core Infrastructure
- **Production Connectors**: 3 operational (Salesforce, MongoDB, FileSource)
- **Canonical Event Processing**: Pydantic models with 147 events validated
- **Schema Drift Detection**: Fingerprinting with Redis-backed caching
- **Auto-Repair System**: LLM-powered with RAG intelligence, Safe Mode operational
- **Database**: PostgreSQL (Supabase) with 10 migrations, HEAD verified (`a01b6d6912e0`)

#### Validated Capabilities
- âœ… **Canonical transformation** bugs fixed (6 mapping files corrected)
- âœ… **Multi-tenant isolation** via tenant_id scoping (String type)
- âœ… **No-RAG fast path** for dev mode (<10s processing)
- âœ… **Database initialization** clean (zero DuplicatePreparedStatement errors)
- âœ… **Zero startup warnings** after Alembic migration fixes

#### AAM Architecture Components
```
Data Sources (Salesforce, MongoDB, FileSource)
    â†“
AAM Connectors (production credentials from Replit Secrets)
    â†“
Canonical Event Normalization (Pydantic validation)
    â†“
Schema Drift Detection (fingerprinting + Redis)
    â†“
LLM/RAG Auto-Repair (with confidence scoring)
    â†“
Redis Streams â†’ DCL Integration
    â†“
Canonical Event Storage (PostgreSQL)
```

---

### âœ… **DCL (Data Connection Layer) - Production Ready**

#### Core Infrastructure  
- **Graph State Management**: Redis-backed with WebSocket real-time updates
- **Multi-Tenant Architecture**: Distributed locking, tenant-scoped state (ENABLED)
- **Performance**: 3x-7x improvement (62s â†’ 9s for 9-source parallel processing)
- **Database**: DuckDB for materialized views, PostgreSQL for persistence

#### Validated Capabilities
- âœ… **Parallel source processing** with async distributed locks
- âœ… **Dynamic WebSocket URLs** (eliminated hardcoded localhost)
- âœ… **Consolidated graph structure** (single "from AAM" parent node)
- âœ… **Feature flags** (Redis-backed, multi-worker, pub/sub broadcasting)
- âœ… **Schema filtering** always returns ontology/agent nodes

#### DCL Architecture Components
```
User Request (via API/WebSocket)
    â†“
JWT Authentication (suspended in dev mode via DCL_AUTH_ENABLED=false)
    â†“
Tenant Isolation (distributed Redis locks)
    â†“
Source Connection (parallel async processing)
    â†“
AI Entity Mapping (LLM-powered with RAG)
    â†“
Graph Generation (nodes + edges)
    â†“
Redis State Cache + WebSocket Broadcast
    â†“
Live Sankey Visualization (React frontend)
```

---

### âœ… **Enterprise Infrastructure - Operational**

#### Distributed Job Processing (Task 6.5)
- **Redis Queue (RQ)**: Production job workers with retry logic
- **RAG Intelligence**: LLM-powered mapping proposals with embeddings
- **TLS/SSL Redis**: Certificate validation, connection pooling
- **Multi-Tenant Auth**: JWT tenant isolation, shared enqueue logic
- **Graceful Degradation**: Error handling, watchdog processes

**Files**: 11 production-ready modules
- `services/mapping_intelligence/job_workers.py` (job processing)
- `services/mapping_intelligence/job_enqueue.py` (shared logic)
- `services/mapping_intelligence/rag_intelligence.py` (RAG integration)

#### Performance Benchmarking (Task 7.3)
- **4 Workload Profiles**: test/small/medium/large (10-1000 connector scale)
- **Validated Throughput**: 7,500 fields/second at large scale
- **Multi-Format Reports**: JSON, CSV, Markdown
- **Accurate Metrics**: Fixed 10x-500x throughput inflation bug

**Script**: `scripts/benchmark_distributed_jobs.py` (380 lines)

#### Multi-Tenant Stress Testing (Task 7.4)
- **20 Comprehensive Tests**: Isolation, fairness, chaos, scaling, lifecycle
- **Validation**: Linear scaling up to 10 tenants, zero interference
- **Chaos Resilience**: Worker crashes, Redis failures, resource exhaustion
- **Automated Cleanup**: Resource management, state teardown

**Suite**: `tests/test_multi_tenant_stress.py` (637 lines)

#### Documentation Suite (Task 7.5)
- **8 Comprehensive Guides**: 452KB total, runtime-verified examples
- **API Reference**: 100+ pages with actual API captures
- **Operational Runbooks**: Database troubleshooting, incident response
- **Deployment Guides**: Production-ready procedures

**Files**: `docs/api/`, `docs/operations/`, `docs/deployment/`, etc.

#### Live Flow Telemetry System (Phase 4) âœ…
- **Real-Time Dashboard**: React component at `/flow-monitor` with dark mode UI
- **Redis Streams Infrastructure**: Three separate streams (aam:flow, dcl:flow, agent:flow)
- **WebSocket Live Streaming**: <1s latency with XREAD BLOCK polling
- **Event Publishing**: FlowEventPublisher library injected across AAM, DCL, and Agent layers
- **RACI Compliance**: Each layer publishes only its own events, telemetry failures isolated
- **Testing**: 10/10 telemetry tests passing, architect-approved for production
- **UI Features**: Three-column layout (Green/Blue/Purple palette), click-through event details, tenant scoping

**Architecture:**
```
AAM Layer â†’ FlowEventPublisher â†’ aam:flow stream
DCL Layer â†’ FlowEventPublisher â†’ dcl:flow stream     â†’ FlowMonitor API â†’ React Dashboard
Agent Layer â†’ FlowEventPublisher â†’ agent:flow stream   â†“ WebSocket /ws/flow-monitor
```

**API Endpoints:**
- `GET /api/v1/flow-monitor` - Snapshot API (XREVRANGE returns newest events)
- `WebSocket /api/v1/ws/flow-monitor` - Live streaming with automatic reconnection
- `POST /api/v1/flow-monitor/demo` - Demo event generator for integration testing

**Files**: `app/telemetry/`, `app/api/v1/flow_monitor.py`, `frontend/src/components/FlowMonitor.tsx`

---

## Validated Performance Metrics (Actual Results)

### AAM Performance
- **Canonical Events**: 147 events persisted (105 opportunities, 15 accounts, 12 contacts, 10 aws_resources, 5 cost_reports)
- **Validation Success**: Zero validation errors across all event types
- **Processing Speed**: <10s in dev mode (no-RAG fast path)
- **Database**: Clean initialization, zero startup errors

### DCL Performance  
- **Source Connection**: 62s â†’ 9s (3x-7x improvement) for 9-source parallel processing
- **Graph Rendering**: 19-33 nodes (depending on source connections)
- **WebSocket Latency**: Real-time updates (<100ms)
- **State Persistence**: Redis-backed with automatic hydration

### Distributed Job Processing
- **Test Workload**: 40 fields/second (2 jobs Ã— 2 connectors Ã— 10 fields)
- **Small Workload**: 375 fields/second (5 jobs Ã— 5 connectors Ã— 15 fields)
- **Medium Workload**: 2,000 fields/second (10 jobs Ã— 10 connectors Ã— 20 fields)
- **Large Workload**: 7,500 fields/second (20 jobs Ã— 15 connectors Ã— 25 fields)

### Multi-Tenant Isolation
- **Tenant Scaling**: Linear performance up to 10 tenants
- **Data Isolation**: Zero cross-tenant data leakage validated
- **Resource Fairness**: Round-robin semaphore scheduling confirmed
- **Chaos Recovery**: System recovers from worker crashes, Redis failures

---

## Architecture Principles (Validated in Production)

### 1. **Foundation-First Architecture** âœ…
**Status**: Complete (Phase 1 - 100%)
- âœ… Single SQLAlchemy Base (consolidated across AAM/DCL/App)
- âœ… Zero sys.path manipulation
- âœ… `pip install -e .` working
- âœ… Circular imports eliminated
- âœ… Alembic seeing all models correctly

### 2. **Multi-Tenant Isolation** âœ…
**Status**: Validated (93.4% test pass rate)
- âœ… Distributed Redis locking (replaces dual STATE_LOCK + ASYNC_STATE_LOCK)
- âœ… Tenant-scoped state management
- âœ… JWT authentication (suspended in dev via DCL_AUTH_ENABLED=false)
- âœ… Zero cross-tenant interference validated

### 3. **Enterprise Observability** âœ…
**Status**: Complete (Phase 4 - 100%)
- âœ… Comprehensive runbooks (8 files, 452KB)
- âœ… Database troubleshooting procedures
- âœ… Performance benchmarking suite
- âœ… Multi-tenant stress testing framework

### 4. **Production Database Architecture** âœ…
**Status**: Operational (zero errors)
- âœ… Supabase PostgreSQL (prioritized over Replit DATABASE_URL)
- âœ… PgBouncer compatibility ensured
- âœ… 10 migrations validated (HEAD = `a01b6d6912e0`)
- âœ… Schema integrity verified (FK/unique constraints present)

### 5. **Redis Infrastructure** âœ…
**Status**: Production-ready (TLS/SSL validated)
- âœ… TLS/SSL encryption with certificate validation
- âœ… Shared client prevents connection pool exhaustion
- âœ… Graceful degradation with watchdog processes
- âœ… Retry logic with exponential backoff

### 6. **Feature Flag System** âœ…
**Status**: Operational (Redis-backed)
- âœ… `USE_AAM_AS_SOURCE` flag (Redis persistence)
- âœ… Multi-worker support (pub/sub broadcasting)
- âœ… Zero downtime migrations (Strangler Fig pattern)
- âœ… Frontend flag: `VITE_CONNECTIONS_V2`

---

## Data Flow (Current Production State)

### End-to-End Data Flow (Validated)
```
1. Data Sources (Salesforce, MongoDB, FileSource)
   â†“ [AAM Connectors with real credentials]
   
2. AAM Canonical Normalization
   â†“ [147 canonical events validated]
   
3. Schema Drift Detection
   â†“ [Redis fingerprinting + LLM auto-repair]
   
4. Redis Streams Integration
   â†“ [AAM â†’ DCL event bus]
   
5. DCL AI Mapping Engine
   â†“ [LLM + RAG intelligence]
   
6. Graph State Management
   â†“ [Redis-backed, tenant-scoped]
   
7. WebSocket Broadcasting
   â†“ [Real-time updates to frontend]
   
8. Live Sankey Visualization
   â†“ [React frontend with dynamic graph]
```

### Storage Architecture (Current)
```
PostgreSQL (Supabase) - Single source of truth
â”œâ”€â”€ AAM Tables
â”‚   â”œâ”€â”€ connections (20 columns, tenant_id scoped)
â”‚   â”œâ”€â”€ canonical_streams (tenant_id String type)
â”‚   â”œâ”€â”€ mapping_registry (10 columns)
â”‚   â”œâ”€â”€ field_mappings (29 columns, FK constraints)
â”‚   â””â”€â”€ idempotency_keys (duplicate prevention)
â”‚
â”œâ”€â”€ DCL Tables  
â”‚   â”œâ”€â”€ tenants (UUID primary key)
â”‚   â”œâ”€â”€ users (tenant_id FK)
â”‚   â””â”€â”€ tasks (tenant_id scoped)
â”‚
â””â”€â”€ Mapping Intelligence Tables
    â”œâ”€â”€ mapping_embeddings (8 columns, tenant_id FK)
    â”œâ”€â”€ mapping_validations (HITL workflow)
    â””â”€â”€ mv_mapping_lineage_grid (materialized view)

Redis (Upstash with TLS/SSL)
â”œâ”€â”€ Graph State Cache (tenant-scoped, WebSocket sync)
â”œâ”€â”€ Feature Flags (Redis-backed, pub/sub)
â”œâ”€â”€ Job Queue (RQ workers, distributed processing)
â”œâ”€â”€ Distributed Locks (tenant isolation)
â””â”€â”€ Schema Fingerprints (drift detection)

DuckDB (In-Process)
â””â”€â”€ Materialized Views (DCL graph queries)
```

---

## Scaling Validation (Actual Results)

### Current Scale (Validated)
- **AAM Connectors**: 3 production, 6 YAML mappings ready (HubSpot, Zendesk, Pipedrive, Dynamics, SAP, NetSuite)
- **DCL Sources**: 9 sources tested (parallel processing validated)
- **Tenants**: 10 concurrent tenants validated (linear scaling)
- **Throughput**: 7,500 fields/second at large workload

### Proven Capabilities
âœ… **Multi-tenant fairness**: Round-robin semaphore scheduling  
âœ… **Chaos resilience**: Recovers from worker crashes, Redis failures  
âœ… **Performance scaling**: 3x-7x improvement on critical paths  
âœ… **Data isolation**: Zero cross-tenant leakage  
âœ… **Clean startup**: Zero errors or warnings

### Path to Enterprise Scale (100-1000 Connectors)
**Foundation Ready** âœ…: Architecture supports scale target
- Distributed job processing operational
- Multi-tenant isolation validated
- Performance benchmarked up to 7,500 fields/second
- Database schema supports unlimited connectors

**Next Required**:
- Generic connector framework (Phase 2 planning)
- RAG intelligence scaling (embedding optimization)
- Connection pool tuning (PostgreSQL, Redis)
- Horizontal scaling validation (100+ tenants)

---

## Known Limitations & Roadmap

### Current Limitations
- **Connector Scale**: 3 production connectors (validated), 6 YAML ready (not activated)
- **RAG Coverage**: Not yet measured (infrastructure ready, embeddings operational)
- **Generic Connectors**: Not implemented (custom connectors only)
- **Monitoring Dashboard**: Not implemented (runbooks complete, Prometheus planned)

### Phase 2 Requirements (Service Decomposition)
- Break monolithic services (>500 line files)
- Eliminate HTTP coupling
- Implement dependency injection
- Remove global state variables

### Phase 3 Requirements (Complete Testing)
- Achieve 80% coverage (currently ~40% with infrastructure at 80%)
- Set up CI/CD pipeline
- Complete integration test automation

### Phase 5 Requirements (Frontend Optimization)
- Reduce bundle size (2MB â†’ <500KB)
- Single state source (remove localStorage coupling)
- Modernize React patterns

---

## Production Readiness Assessment

### âœ… **Ready for Production Use**
- Multi-tenant architecture validated
- Database migrations clean and verified
- Performance benchmarked and optimized
- Complete operational documentation
- Zero startup errors or warnings
- Distributed job processing operational

### âš ï¸ **Recommended Before Full Scale**
- Service decomposition (Phase 2) for maintainability
- CI/CD pipeline for automated testing
- Frontend bundle optimization for load times
- Generic connector framework for rapid expansion

### ðŸŽ¯ **Current Status: BETA-READY**
**Estimated Path to Full Production**: 6-8 weeks (completing Phases 2, 3, 5)

---

# ðŸ“‹ ORIGINAL PLANNING DOCUMENT (November 17, 2025)

*The sections below represent the original architecture planning and aspirational targets. See "CURRENT PRODUCTION STATE" above for actual achieved results.*

---

## Executive Summary (Original Planning)

**Challenge:** Scale AutonomOS Adaptive API Mesh (AAM) to handle 100s-1000s of data connectors in production while maintaining intelligent auto-connection and auto-mapping with minimal human intervention.

**Critical Finding:** Initial plan would NOT scale to enterprise requirements due to:
- Unsustainable LLM costs ($500 per onboarding cycle for 1000 connectors)
- Multi-hour onboarding latency (27+ hours sequential)
- YAML file management doesn't scale (1000s of git files)
- Hand-coded connectors require 500 lines per connector
- No multi-tenant resource isolation

**Solution:** Enterprise architecture with 6 core principles:
1. RAG-First, LLM-Last (95% RAG, 5% LLM) â†’ 20x cost reduction
2. Database-Backed Mapping Registry â†’ Runtime updates, versioning, audit trail
3. Generic Connector Framework â†’ Config-driven, not hand-coded
4. Distributed Event-Driven Workers â†’ Parallel processing, auto-scaling
5. Multi-Tenant Resource Isolation â†’ Quotas, rate limiting, table partitioning
6. Enterprise Observability â†’ OpenTelemetry, per-tenant metrics

**Realistic Impact (Validated):**
- Onboarding time: 27 hours â†’ 2 hours (13x faster) | Stretch Goal: 30 minutes
- LLM costs: $500 â†’ $100 per cycle (5x cheaper) | Stretch Goal: $50
- Code per connector: 500 lines â†’ 100 line JSON config (where applicable)
- Scalability: 10-20 connectors â†’ 1000+ connectors
- RAG hit rate: 0% â†’ 85% (realistic) | Stretch Goal: 95%

**Note:** Initial claims (162x faster, 20x cheaper) were aspirational projections. These revised targets are based on enterprise architecture review and represent achievable production goals with proper implementation.

---

## Context: Initial Plan

### Objectives
1. **Expand AAM Connections:** Increase from 4 â†’ 10+ active connectors across equal domains (RevOps, FinOps, SupportOps, MarketingOps)
2. **Build Monitoring Dashboard:** Real-time visibility into auto-connection, auto-mapping coverage, HITL review queue, LLM proposals

### Proposed Connector Expansion

**Priority 1: Activate Existing YAML Mappings (8 available)**
- HubSpot (RevOps) - YAML exists
- Zendesk (SupportOps) - YAML exists
- Pipedrive (RevOps) - YAML exists
- Dynamics 365 (RevOps) - YAML exists

**Priority 2: Add New Entity Types**
- Stripe (FinOps) - Need YAML
- Intercom (MarketingOps) - Need YAML
- Google Analytics (MarketingOps) - Need YAML
- AWS CloudWatch (FinOps) - Partial YAML

**Target:** 10-12 active connectors demonstrating cross-domain unification

### Proposed Monitoring Dashboard

**7 Core Components:**
1. Service Status Panel (Schema Observer, RAG Engine, Drift Repair Agent)
2. Key Metrics Cards (Connections, Drift Events, Auto-Repairs, Avg Confidence)
3. Connection Health Table (Status, Last Sync, Events/hr, Coverage %)
4. Drift Detection Timeline (7-day time-series)
5. Mapping Coverage Chart (Bar chart by connector)
6. Auto-Repair Activity Feed (Real-time repair log)
7. Canonical Event Stats (Entity breakdown donut chart)

---

## Scalability Analysis: Why Initial Plan Fails at Enterprise Scale

### Issue 1: LLM-per-Field Approach (Cost/Latency Explosion)

**Problem:**
```
1000 connectors Ã— 50 fields avg = 50,000 LLM calls
Cost: $0.01/call Ã— 50,000 = $500 per onboarding cycle
Latency: 2s/call Ã— 50,000 = 27 hours sequential
```

**Why This Fails:**
- âŒ Unsustainable cost at scale ($6,000/year for monthly re-scanning)
- âŒ Multi-hour onboarding time blocks user productivity
- âŒ LLM rate limits (10k requests/min) throttle concurrent onboarding
- âŒ No way to batch/parallelize effectively with sequential field processing

---

### Issue 2: YAML Files Don't Scale

**Problem:**
```
services/aam/canonical/mappings/
â”œâ”€â”€ connector1.yaml
â”œâ”€â”€ connector2.yaml
â”œâ”€â”€ ... (1000 files)
â””â”€â”€ connector1000.yaml
```

**Why This Fails:**
- âŒ Git repo bloat (1000s of files, slow clones/pulls)
- âŒ No versioning/rollback per mapping (file replace only)
- âŒ No approval workflow (manual PR process)
- âŒ Merge conflicts at scale (multiple teams editing mappings)
- âŒ No runtime updates (requires redeploy to add mapping)
- âŒ No A/B testing (can't test two mapping versions simultaneously)

---

### Issue 3: Hand-Coded Connectors Don't Scale

**Problem:**
```python
class SalesforceConnector:  # 500 lines
    def __init__(self): ...
    def fetch_opportunities(self): ...
    def normalize_field(self): ...
    # ... repeat for every entity

class HubSpotConnector:  # 500 lines
    # Duplicate 90% of Salesforce logic
    # Only API URLs differ

# Repeat 1000 times? NO.
```

**Why This Fails:**
- âŒ Can't hand-code 1000 connectors (50,000 lines of code)
- âŒ Maintenance nightmare (bug fix = update 1000 files)
- âŒ No standardization (each connector implements auth differently)
- âŒ Slow to add new connectors (3-5 days per connector)
- âŒ Testing burden (unit tests Ã— 1000 connectors)

---

### Issue 4: Synchronous Schema Scanning

**Problem:**
```python
for connector in connectors:  # 1000 iterations
    schema = await scan_schema(connector)  # 5s each
    mappings = await propose_mappings(schema)  # 10s each
# Total: 15s Ã— 1000 = 15,000s = 4.2 hours
```

**Why This Fails:**
- âŒ Hours to scan all connectors on system startup
- âŒ Blocks other operations (no concurrency)
- âŒ Single point of failure (one connector timeout = entire scan fails)
- âŒ No priority queuing (critical connectors wait behind slow ones)

---

### Issue 5: No Multi-Tenant Resource Isolation

**Problem:**
- All tenants share same connector pool
- No quotas, no rate limiting
- One tenant can DOS entire system
- No SLA guarantees per tenant

**Why This Fails:**
- âŒ Noisy neighbor problem (Tenant A's 1000 connectors slow Tenant B)
- âŒ No SLA guarantees (can't promise uptime per tenant)
- âŒ Security/compliance risk (tenant data not isolated)
- âŒ No billing fairness (all tenants pay same despite usage)

---

## Responsibility Allocation (AOD, AAM, DCL)

**Critical Finding:** AAM is becoming bloated with overlapping responsibilities. Clear separation needed.

### Component Responsibility Matrix (RACI)

| Capability | AOD | AAM | DCL | Notes |
|------------|-----|-----|-----|-------|
| **Discovery & Cataloging** |||||
| Asset discovery (find data sources) | **A/R** | C | I | AOD owns discovery |
| Schema introspection | I | **A/R** | C | AAM owns runtime schema scanning |
| Source catalog management | **A/R** | I | C | AOD owns catalog |
| **Connection Runtime** |||||
| API authentication/OAuth | I | **A/R** | I | AAM owns credential management |
| Data fetching/sync | I | **A/R** | I | AAM owns data retrieval |
| Pagination handling | I | **A/R** | I | AAM owns API pagination |
| Rate limiting/throttling | I | **A/R** | I | AAM owns API limits |
| Connection health monitoring | C | **A/R** | I | AAM owns monitoring |
| **Intelligence & Mapping** |||||
| LLM-powered mapping proposals | - | I | **A/R** | DCL owns LLM decisions |
| RAG mapping lookup | - | I | **A/R** | DCL owns RAG system |
| Mapping registry storage | - | C | **A/R** | DCL owns registry database |
| Mapping approval workflow | - | I | **A/R** | DCL owns approvals |
| Confidence scoring | - | I | **A/R** | DCL owns scoring logic |
| **Transformation** |||||
| Canonical transformation | - | R | **A** | DCL decides, AAM executes |
| Schema drift detection | - | **A/R** | C | AAM detects and owns |
| Drift repair proposals | - | I | **A/R** | DCL owns repair logic |
| **Graph & Ontology** |||||
| Ontology management | - | - | **A/R** | DCL fully owns |
| Graph generation | - | I | **A/R** | DCL owns graph building |
| Entity resolution | - | I | **A/R** | DCL owns resolution |
| **Agent Orchestration** |||||
| Agent execution | - | - | **A/R** | DCL owns execution |
| Agent context management | - | - | **A/R** | DCL owns context |
| Multi-agent coordination | - | - | **A/R** | DCL owns coordination |

**RACI Legend (Corrected):**
- **A** = Accountable (single owner, ultimately answerable)
- **R** = Responsible (does the work, can be multiple)
- **C** = Consulted (provides input, two-way communication)
- **I** = Informed (kept updated, one-way communication)

**Key Rule:** Each capability has exactly ONE Accountable owner (marked as A or A/R when same component is both)

### Orchestration Boundaries (DCL vs AOA)

**DCL Scope:**
- Owns agent context/data and low-level actions within data domain
- Entity mapping, graph generation, single-agent execution
- Data transformation decisions and confidence scoring

**AOA/xAO Scope (Out of Document Scope):**
- Higher-level workflow orchestration
- Cross-domain playbooks (e.g., data + compute + network)
- Multi-agent coordination across different domains
- Business process automation spanning multiple systems

---

## Functional Data Flow Architecture

### System Data Flow Overview

```mermaid
graph TB
    subgraph "Discovery Layer"
        U[User] -->|Requests Discovery| AOD[AOD Service]
        AOD -->|Catalog Sources| PC[(Provider Catalog)]
    end
    
    subgraph "Connection Layer"
        AOD -->|Notify Available| AAM[AAM Workers]
        AAM -->|Get Credentials| IM[Integration Manager]
        IM -->|Encrypted Secrets| V[(Vault/DB)]
        AAM -->|Fetch Data| EXT[External APIs]
    end
    
    subgraph "Intelligence Layer"
        AAM -->|Request Mapping| DCL[DCL Engine]
        DCL -->|RAG Lookup| PIN[(Pinecone)]
        DCL -->|LLM Fallback| LLM[LLM Service]
        DCL -->|Store Mappings| PG[(PostgreSQL)]
    end
    
    subgraph "Processing Layer"
        AAM -->|Raw Data| RDS[(Redis Queue)]
        RDS -->|Batch Process| DUCK[(DuckDB)]
        DUCK -->|Transform| AAM
        AAM -->|Canonical Events| PG
    end
    
    subgraph "Orchestration Layer"
        DCL -->|Execute| AGT[Agents]
        AGT -->|Read Context| PG
        AGT -->|Write Results| PG
        AGT -->|Notifications| U
    end
```

### Detailed Data Flows by Operation

#### 1. Source Discovery Flow (AOD)
```
User Request â†’ AOD Service â†’ Provider Scan â†’ Catalog Update â†’ Notification to AAM
```
**Data Storage:**
- **PostgreSQL**: Provider catalog, source metadata
- **Redis**: Discovery status cache

#### 2. Connection Establishment Flow (AAM)
```
AAM Worker â†’ Integration Manager â†’ Vault (Get Credentials) â†’ OAuth/API Auth â†’ External API
```
**Data Storage:**
- **PostgreSQL**: Connection metadata, auth tokens
- **Vault**: Encrypted credentials per tenant
- **Redis**: Token refresh queue

#### 3. Data Fetching & Transformation Flow (AAM)
```
External API â†’ AAM Worker â†’ Redis Queue â†’ DuckDB Processing â†’ Canonical Transform â†’ PostgreSQL
```
**Data Storage:**
- **Redis**: Job queue, rate limit state
- **DuckDB**: Ephemeral processing (joins, aggregations)
- **PostgreSQL**: Canonical events, audit log

#### 4. Mapping Intelligence Flow (DCL)
```
AAM Request â†’ DCL â†’ Pinecone (RAG) â†’ PostgreSQL (Mappings) â†’ Response to AAM
```
**Flow Details:**
1. **AAM sends unknown field**: `{"field": "cust_nm", "value": "John Doe", "context": "contact"}`
2. **DCL RAG lookup**: Query Pinecone for similar fields
3. **High confidence (>0.85)**: Return RAG result immediately
4. **Low confidence (<0.85)**: Fall back to LLM for proposal
5. **Store & return**: Save to PostgreSQL, return to AAM

**Data Storage:**
- **Pinecone**: Vector embeddings for fields/schemas
- **PostgreSQL**: Mapping registry (source of truth)
- **Redis**: Hot mapping cache (TTL: 5 minutes)

#### 5. Agent Execution Flow (DCL â†’ Agents)
```
User Query â†’ DCL â†’ Graph Generation â†’ Agent Selection â†’ Execution â†’ Results
```
**Data Storage:**
- **PostgreSQL**: Agent definitions, execution logs, results
- **DuckDB**: Query planning and optimization
- **Redis**: Execution state pub/sub

### Data Storage Responsibilities

| Component | PostgreSQL (Supabase) | Redis | DuckDB | Pinecone |
|-----------|----------------------|-------|---------|----------|
| **AOD** | Provider catalog, source metadata | Discovery cache | - | - |
| **AAM** | Connections, canonical events | Job queues, rate limits | Batch transforms | - |
| **DCL** | Mappings, ontology, agent data | Mapping cache | Query planning | Field embeddings |
| **Agents** | Execution logs, results, context | State notifications | Analytics queries | - |

### Multi-Tenant Data Isolation

```mermaid
graph LR
    T1[Tenant 1] -->|Scoped| D1[Data Partition 1]
    T2[Tenant 2] -->|Scoped| D2[Data Partition 2]
    T3[Tenant 3] -->|Scoped| D3[Data Partition 3]
    
    D1 --> PG[(PostgreSQL)]
    D2 --> PG
    D3 --> PG
    
    D1 --> NS1[Pinecone Namespace 1]
    D2 --> NS2[Pinecone Namespace 2]
    D3 --> NS3[Pinecone Namespace 3]
```

**Enforcement Points:**
1. **PostgreSQL**: All queries include `WHERE tenant_id = ?`
2. **Redis**: Keys prefixed with `tenant:{id}:`
3. **Pinecone**: Namespace per tenant or metadata filter
4. **DuckDB**: Ephemeral, tenant data never mixed

### Event-Driven Communication

```mermaid
sequenceDiagram
    participant AAM
    participant Redis
    participant DCL
    participant Agent
    participant User
    
    AAM->>Redis: Publish canonical event
    Redis->>DCL: Event notification
    DCL->>DCL: Process event
    DCL->>Agent: Trigger if relevant
    Agent->>Agent: Execute action
    Agent->>Redis: Publish result
    Redis->>User: SSE/WebSocket update
```

### Performance & Scaling Data Paths

**Hot Path (Cached):**
```
AAM â†’ Redis Cache â†’ Immediate Response (< 10ms)
```

**Warm Path (RAG):**
```
AAM â†’ DCL â†’ Pinecone â†’ PostgreSQL â†’ Response (< 100ms)
```

**Cold Path (LLM):**
```
AAM â†’ DCL â†’ LLM â†’ PostgreSQL â†’ Response (< 2s)
```

**Batch Path (Analytics):**
```
PostgreSQL â†’ DuckDB â†’ Aggregation â†’ Results (async, minutes)
```

### Critical Data Guarantees

1. **PostgreSQL is the ONLY source of truth** - All authoritative data lives here
2. **Redis is ONLY for ephemeral data** - Can be flushed without data loss
3. **DuckDB is ONLY for processing** - No persistent state
4. **Pinecone is ONLY for search** - PostgreSQL holds actual mappings

### Failure & Recovery Flows

**Component Failure Scenarios:**

| Failure | Impact | Recovery | Data Loss |
|---------|--------|----------|-----------|
| PostgreSQL down | System halt | Failover replica | None (replicated) |
| Redis down | Degraded performance | Use PostgreSQL directly | None (ephemeral) |
| DuckDB crash | Analytics unavailable | Restart worker | None (ephemeral) |
| Pinecone down | Slow mappings (LLM fallback) | Use PostgreSQL search | None (cache only) |
| AAM worker crash | Connection paused | Redis requeue job | None (idempotent) |
| DCL service down | No new mappings | Use cached mappings | None (cache fallback) |

---

### Refactored Architecture (Preventing AAM Bloat)

**AOD (AOS Discover) - Discovery Front Door:**
```python
# PSEUDOCODE - Architectural example
class AODService:
    responsibilities = [
        "Discover available data sources",
        "Maintain source catalog",
        "Scan for new sources",
        "Classify source types"
    ]
    
    # Simple, clean interface
    async def discover_sources() -> List[DataSource]
    async def catalog_source(source: DataSource)
    async def get_source_metadata(source_id: str)
```

**AAM (Adaptive API Mesh) - Runtime Transport Layer:**
```python
# PSEUDOCODE - Architectural example
class AAMService:
    responsibilities = [
        "Manage API connections (auth, rate limits)",
        "Fetch data from sources",
        "Apply canonical transformations (using DCL mappings)",
        "Detect schema drift (report to DCL)",
        "Emit canonical events"
    ]
    
    # AAM doesn't own LLM/RAG - it queries DCL
    async def get_mapping(field: str) -> MappingResult:
        # Delegate to DCL's intelligence layer
        return await dcl_client.get_mapping(field)
    
    # AAM applies transformations, doesn't decide them
    async def transform_to_canonical(data: dict) -> dict:
        mappings = await dcl_client.get_mappings(self.source_type)
        return self.apply_mappings(data, mappings)
```

**DCL (Data Connection Layer) - Intelligence & Orchestration:**
```python
# PSEUDOCODE - Architectural example
class DCLService:
    responsibilities = [
        "LLM-powered mapping proposals",
        "RAG mapping lookups",
        "Mapping registry (database)",
        "Mapping approval workflow",
        "Ontology management",
        "Graph generation",
        "Agent orchestration",
        "Drift repair decisions"
    ]
    
    # DCL owns all LLM calls
    async def propose_mapping(field: str) -> MappingProposal:
        # Try RAG first (95% success)
        rag_result = await self.rag.lookup(field)
        if rag_result.confidence > 0.85:
            return rag_result
        
        # Fall back to LLM (5% of cases)
        return await self.llm.propose_mapping(field)
    
    # DCL owns the mapping registry
    async def store_mapping(mapping: MappingProposal):
        await self.db.insert_mapping(mapping)
```

### Why This Prevents Bloat

**Before (Bloated AAM):**
- AAM: 8 responsibilities (connections + intelligence + transformation)
- DCL: 5 responsibilities (some overlap with AAM)
- Unclear ownership of LLM/RAG

**After (Clean Separation):**
- AOD: 4 responsibilities (discovery only)
- AAM: 5 responsibilities (runtime transport only)
- DCL: 8 responsibilities (all intelligence consolidated)

**Key Principle:** AAM is a "dumb pipe" that moves data. DCL is the "smart brain" that decides how.

---

## Integration Manager & Connector Toolkit

**All runtime behavior, credentials, and multi-tenancy are owned by our own services and infrastructure**

### Integration Manager (Internal Service)

**Purpose:** Centralized management of provider integrations and tenant connections

```python
# Real Production Runtime API
class IntegrationManager:
    """Internal service for managing all provider connections"""
    
    def __init__(self):
        self.catalog = ProviderCatalog()  # Maintains registry of available providers
        self.vault = SecretsInfrastructure()  # Our own secrets management
        self.auth_manager = OAuthManager()  # Handles OAuth flows
    
    def register_tenant_connection(self, tenant_id: str, provider: str, credentials: dict):
        """Store per-tenant connection with encrypted credentials"""
        encrypted = self.vault.encrypt(credentials, tenant_id)
        self.catalog.register(tenant_id, provider, encrypted)
    
    def get_connection_for_worker(self, tenant_id: str, provider: str):
        """AAM workers call this to get decrypted credentials"""
        encrypted = self.catalog.get(tenant_id, provider)
        return self.vault.decrypt(encrypted, tenant_id)
    
    def handle_oauth_flow(self, provider: str, tenant_id: str):
        """Complete OAuth flow and store tokens"""
        tokens = self.auth_manager.complete_flow(provider)
        self.register_tenant_connection(tenant_id, provider, tokens)
```

**Key Responsibilities:**
- Maintains provider catalog and per-tenant connections
- Handles OAuth/API-key flows internally
- Stores credentials in our secrets infrastructure (Vault/DB)
- Exposes simple API to AAM workers for fetching credentials
- Manages token refresh and credential rotation

### Connector Toolkit (Development Side)

**Purpose:** Accelerate connector development with templates and tools

```python
# Connector Base Template
class BaseConnector(ABC):
    """Templated skeleton for all connectors"""
    
    def __init__(self, tenant_id: str):
        # Get credentials from Integration Manager
        self.creds = integration_manager.get_connection_for_worker(
            tenant_id, self.provider_name
        )
        self.setup_logging()
        self.setup_error_handling()
        self.setup_rate_limiting()
    
    @abstractmethod
    async def fetch_data(self, params: dict):
        """Provider-specific implementation"""
        pass
    
    @retry_with_backoff
    @rate_limit
    async def execute(self, operation: str, params: dict):
        """Standard execution wrapper with retry and rate limiting"""
        try:
            return await self.fetch_data(params)
        except ProviderError as e:
            self.handle_provider_error(e)
```

**Development Tools:**
- **CLI for Scaffolding:** `aos-connector create --provider stripe --type payment`
- **Base Classes:** Pre-built error handling, logging, retry logic
- **Test Harness:** Standard test suite for all connectors
- **CI/CD Pipeline:** Automated testing and deployment

### AI-Assisted Development Workflow

We use AI-native development tools to scaffold connectors, but all runtime behavior, credentials, and multi-tenancy are owned by our own services and infrastructure.

**Example Workflow:**
1. **Generate Skeleton:** CLI creates connector from template
2. **AI-Assist Implementation:** Use ChatGPT/Claude to generate provider-specific logic
3. **Standard Tests:** Run connector through test harness
4. **Deploy:** CI/CD pipeline validates and ships to production
5. **Runtime:** Integration Manager handles all credentials and auth

### Production Architecture

```mermaid
graph TD
    T[Tenant] -->|OAuth/API Key| IM[Integration Manager]
    IM -->|Store| V[Vault/DB]
    IM -->|Manage| PC[Provider Catalog]
    
    AAM[AAM Worker] -->|Request Creds| IM
    IM -->|Decrypt & Return| AAM
    AAM -->|Fetch Data| API[External API]
    
    Dev[Developer] -->|Use| CT[Connector Toolkit]
    CT -->|Generate| BC[Base Connector]
    BC -->|Deploy| AAM
```

**Key Points:**
- No external service handles our tenant secrets
- No external auth in the critical path
- All multi-tenancy enforced by our infrastructure
- Connectors are just code templates, not runtime dependencies

---

## Enterprise Architecture: Production-Ready Design

### Principle 1: RAG-First, LLM-Last (DCL Responsibility)

**Important:** Based on responsibility allocation, this belongs in DCL, not AAM

**Strategy:** 95% of mappings from RAG (cheap, instant), 5% from LLM (expensive, slow)

**Implementation:**
```python
# PSEUDOCODE - Conceptual flow
async def propose_mapping(field_name: str, field_type: str):
    # Step 1: RAG lookup (99% of the time this works)
    rag_results = await rag.retrieve_similar_mappings(
        field_name=field_name,
        field_type=field_type,
        similarity_threshold=0.85  # High bar
    )
    
    if rag_results and rag_results[0].similarity > 0.85:
        # RAG has high-confidence match - USE IT
        return MappingProposal(
            source=field_name,
            target=rag_results[0].canonical_field,
            confidence=rag_results[0].similarity,
            method="rag",
            cost=0.0001  # Nearly free
        )
    
    # Step 2: LLM only for truly novel fields (1% of cases)
    llm_proposal = await llm.propose_mapping(field_name, field_type)
    
    # Step 3: Store LLM result in RAG for future reuse
    await rag.store_mapping(llm_proposal)
    
    return llm_proposal
```

**Realistic Impact:**
```
Before: 50,000 LLM calls = $500
After:  10,000 LLM calls (15-20% initially) = $100 (5x cheaper)
        Target after 100 connectors: 2,500 calls (5%) = $50

Before: 27 hours sequential
After:  RAG lookups in parallel = 2 hours (13x faster)
        Stretch goal with optimization: 30 minutes
```

**Key Insight:** After the first 100 connectors, RAG hit rate approaches 85-95% because most CRM/ERP systems use similar field names (`customer_id`, `email`, `name`, etc.)

**Critical: RAG Governance Required**
```python
class RAGGovernance:
    """Production RAG requires quality gates and tenant isolation"""
    
    async def validate_mapping_quality(self, mapping: MappingProposal):
        # Quality threshold enforcement
        if mapping.confidence < 0.85:
            # Low confidence - needs human review
            await self.queue_for_hitl_review(mapping)
            return ValidationResult.NEEDS_HUMAN_REVIEW
        
        # Tenant isolation in vector space
        tenant_namespace = f"tenant_{mapping.tenant_id}_mappings"
        
        # Cold-start handling (new tenant with no training data)
        training_data_count = await self.count_tenant_mappings(tenant_namespace)
        if training_data_count < 50:
            # Use global baseline mappings until tenant has enough data
            return await self.use_global_baseline(mapping)
        
        # Detect conflicting mappings (same source field â†’ different canonical fields)
        conflicts = await self.check_conflicting_mappings(mapping)
        if conflicts:
            await self.escalate_conflict(mapping, conflicts)
            return ValidationResult.CONFLICT_DETECTED
        
        return ValidationResult.APPROVED
    
    async def store_validated_mapping(self, mapping: MappingProposal):
        """Store in RAG with tenant isolation"""
        # Create embedding with tenant namespace
        embedding = await self.create_embedding(
            text=f"{mapping.source_field}:{mapping.field_type}",
            metadata={
                "tenant_id": mapping.tenant_id,
                "confidence": mapping.confidence,
                "canonical_field": mapping.canonical_field
            }
        )
        
        # Store in pgvector with tenant-scoped index
        await self.vector_store.insert(
            namespace=f"tenant_{mapping.tenant_id}",
            embedding=embedding
        )
```

**Tenant-Scoped Vector Indexes (Required for Multi-Tenancy):**
```sql
-- Create pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Mapping embeddings table with tenant isolation
CREATE TABLE mapping_embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR NOT NULL,
    source_field VARCHAR NOT NULL,
    field_type VARCHAR,
    canonical_field VARCHAR NOT NULL,
    embedding vector(1536),  -- OpenAI ada-002 dimension
    confidence FLOAT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Per-tenant vector index for fast similarity search
CREATE INDEX idx_embeddings_tenant_vector 
ON mapping_embeddings USING ivfflat (embedding vector_cosine_ops)
WHERE tenant_id = 'current_tenant';  -- Partition by tenant

-- Index for tenant lookup
CREATE INDEX idx_embeddings_tenant ON mapping_embeddings(tenant_id);
```

---

### Principle 2: Database-Backed Mapping Registry (DCL Responsibility)

**Important:** Mapping registry belongs in DCL (intelligence layer), not AAM (transport layer)

**Replace YAML with PostgreSQL:**

```sql
CREATE TABLE mapping_registry (
    id UUID PRIMARY KEY,
    source_system VARCHAR NOT NULL,
    source_entity VARCHAR NOT NULL,
    source_field VARCHAR NOT NULL,
    canonical_entity VARCHAR NOT NULL,
    canonical_field VARCHAR NOT NULL,
    
    confidence FLOAT NOT NULL,
    method VARCHAR NOT NULL,  -- 'rag', 'llm', 'manual'
    
    approved_by VARCHAR,      -- User who approved
    approved_at TIMESTAMP,
    
    version INT NOT NULL DEFAULT 1,
    is_active BOOLEAN DEFAULT TRUE,
    
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(source_system, source_entity, source_field, version)
);

CREATE INDEX idx_mapping_lookup 
ON mapping_registry(source_system, source_entity, source_field) 
WHERE is_active = TRUE;
```

**Benefits:**
- âœ… Version control (rollback any mapping to previous version)
- âœ… Approval workflow built-in (track who approved what)
- âœ… Runtime updates (no redeploy needed)
- âœ… Fast lookups (<1ms with index)
- âœ… Audit trail (compliance requirement)
- âœ… A/B testing (version 1 vs version 2 comparison)

**Example Query:**
```sql
-- Get current active mapping
SELECT canonical_field, confidence, method
FROM mapping_registry
WHERE source_system = 'stripe' 
  AND source_entity = 'customer'
  AND source_field = 'email'
  AND is_active = TRUE
LIMIT 1;
```

**Critical: Migration Strategy from YAML to Database**

**Current State:** `services/aam/canonical/mapping_registry.py` uses YAML files
**Target State:** PostgreSQL-backed registry with version control

**3-Phase Migration Plan:**

```python
# Phase 1: Shadow Mode (Dual-Write, No Production Reads)
class MigrationPhase1:
    """Write to both YAML and database, validate consistency"""
    
    async def save_mapping(self, mapping: MappingProposal):
        # Continue writing to YAML (production path)
        await self.yaml_writer.save(mapping)
        
        # Also write to new database (shadow path)
        await self.db_writer.save(mapping)
        
        # Validation: Compare outputs
        yaml_result = await self.yaml_reader.get_mapping(mapping.source_field)
        db_result = await self.db_reader.get_mapping(mapping.source_field)
        
        if yaml_result != db_result:
            await self.alert_discrepancy(yaml_result, db_result)
    
    # Duration: Run for sufficient validation period
    # Success: Zero discrepancies for sustained period
```

```python
# Phase 2: Canary Deployment (Gradual Production Reads)
class MigrationPhase2:
    """Gradually shift production reads to database"""
    
    async def get_mapping(self, source_field: str):
        # Feature flag controls percentage
        rollout_pct = await self.get_rollout_percentage()  # 5% â†’ 25% â†’ 50% â†’ 100%
        
        if random.random() < (rollout_pct / 100):
            # Read from database (canary traffic)
            result = await self.db_reader.get_mapping(source_field)
            
            # Shadow read from YAML for comparison
            yaml_result = await self.yaml_reader.get_mapping(source_field)
            
            if result != yaml_result:
                # Discrepancy detected - fallback to YAML
                await self.alert_discrepancy(result, yaml_result)
                return yaml_result
            
            return result
        else:
            # Read from YAML (existing traffic)
            return await self.yaml_reader.get_mapping(source_field)
    
    # Rollout: 5% â†’ 25% â†’ 50% â†’ 100% over several phases
    # Rollback: Feature flag flip to 0% instantly
```

```python
# Phase 3: Full Cutover (Database Primary, YAML Deprecated)
class MigrationPhase3:
    """Database is production, YAML read-only archive"""
    
    async def get_mapping(self, source_field: str):
        # Always read from database
        return await self.db_reader.get_mapping(source_field)
    
    async def deprecate_yaml(self):
        # Move YAML files to archive
        await self.archive_yaml_files(
            source_path="services/aam/canonical/mappings/",
            archive_path="services/aam/canonical/mappings_archive/"
        )
        
        # Keep YAML files for 90 days (rollback window)
        # After 90 days, safe to delete
    
    # Success: 100% database reads, zero YAML writes
    # Rollback window: 90 days with archived YAML
```

**Migration Validation Checklist:**
- [ ] Phase 1: All YAML mappings migrated to database with version=1
- [ ] Phase 1: Dual-write operational (both YAML and DB updated)
- [ ] Phase 1: Zero discrepancies observed for sustained period
- [ ] Phase 2: Canary deployment at 5% with monitoring
- [ ] Phase 2: Gradual rollout to 25%, 50%, 100%
- [ ] Phase 2: Rollback tested (feature flag â†’ 0% works instantly)
- [ ] Phase 3: 100% database reads confirmed
- [ ] Phase 3: YAML files archived and deprecated

---

### Principle 3: Generic Connector Framework (AAM Responsibility)

**Important:** This stays in AAM (connection runtime), but AAM delegates mapping intelligence to DCL

**Reality Check: Not All Connectors Can Be Generic**

**Current State:** `services/aam/connectors/salesforce/connector.py` has 500+ lines of custom logic:
- OAuth2 refresh flows
- Salesforce-specific pagination (`nextRecordsUrl`)
- Bulk API vs REST API decisions
- Field-level security checks
- Custom query language (SOQL)

**You cannot config-drive this complexity away.**

**Realistic Approach: Capability Matrix**

```python
class ConnectorCapabilityMatrix:
    """Define what CAN be generic vs what needs custom code"""
    
    GENERIC_CAPABILITIES = {
        "simple_rest": {
            "description": "Standard REST APIs with simple auth",
            "requirements": [
                "REST/HTTP based",
                "Standard auth (API key, Bearer token, Basic)",
                "Simple pagination (offset, cursor, page)",
                "JSON responses",
                "No custom query language"
            ],
            "examples": ["Stripe", "GitHub", "Slack", "Twilio"],
            "connector_type": "GenericRESTConnector",
            "config_lines": 100,
            "code_lines": 0
        },
        "openapi_introspectable": {
            "description": "APIs with OpenAPI specs for auto-discovery",
            "requirements": [
                "OpenAPI 3.0+ specification available",
                "Standard HTTP methods",
                "Schema introspection supported"
            ],
            "examples": ["Stripe", "GitHub", "Plaid"],
            "connector_type": "OpenAPIConnector",
            "config_lines": 50,
            "code_lines": 0
        }
    }
    
    CUSTOM_REQUIRED = {
        "salesforce": {
            "description": "Enterprise CRM with custom APIs",
            "complexity_drivers": [
                "SOQL custom query language",
                "Bulk API vs REST API logic",
                "Field-level security",
                "OAuth2 with refresh token rotation",
                "nextRecordsUrl pagination"
            ],
            "connector_type": "SalesforceConnector (custom)",
            "config_lines": 100,
            "code_lines": 500
        },
        "dynamics365": {
            "description": "Microsoft enterprise suite",
            "complexity_drivers": [
                "OData query protocol",
                "Batch request handling",
                "Azure AD authentication",
                "Complex entity relationships"
            ],
            "connector_type": "DynamicsConnector (custom)",
            "config_lines": 100,
            "code_lines": 400
        },
        "sap": {
            "description": "ERP with proprietary protocols",
            "complexity_drivers": [
                "RFC/BAPI protocols",
                "IDoc messaging",
                "SAP-specific authentication",
                "ABAP function modules"
            ],
            "connector_type": "SAPConnector (custom)",
            "config_lines": 150,
            "code_lines": 800
        }
    }
    
    HYBRID_APPROACH = {
        "hubspot": {
            "description": "Mostly generic with custom pagination",
            "base_connector": "GenericRESTConnector",
            "custom_adapters": ["PaginationAdapter", "RateLimitAdapter"],
            "config_lines": 100,
            "code_lines": 150
        }
    }
    
    @classmethod
    def estimate_connector_cost(cls, api_name: str) -> dict:
        """Estimate development cost for new connector"""
        if api_name in cls.GENERIC_CAPABILITIES.get("examples", []):
            return {
                "approach": "generic",
                "config_lines": 100,
                "code_lines": 0,
                "dev_effort": "1-2 hours (config only)"
            }
        elif api_name in cls.CUSTOM_REQUIRED:
            custom = cls.CUSTOM_REQUIRED[api_name]
            return {
                "approach": "custom",
                "config_lines": custom["config_lines"],
                "code_lines": custom["code_lines"],
                "dev_effort": "3-5 days (custom implementation)"
            }
        else:
            return {
                "approach": "unknown - requires analysis",
                "recommendation": "Test with GenericRESTConnector first, add custom code if needed"
            }
```

**Realistic Generic Connector (for ~40% of APIs):**

```python
class GenericRESTConnector:
    """Works for simple REST APIs only"""
    
    SUPPORTS = [
        "Standard REST endpoints",
        "JSON responses",
        "API key or Bearer token auth",
        "Offset/cursor pagination",
        "Rate limiting (token bucket)"
    ]
    
    DOES_NOT_SUPPORT = [
        "Custom query languages (SOQL, OData)",
        "Complex auth flows (SAML, custom OAuth)",
        "Binary protocols (gRPC, SOAP)",
        "Batch/bulk APIs",
        "Field-level permissions"
    ]
    
    def __init__(self, config: ConnectorConfig):
        self.config = config
    
    async def sync_entity(self, entity_name: str):
        """Generic sync logic"""
        endpoint = self.config.entities[entity_name]
        
        async for page in self._paginate(endpoint):
            for record in page:
                canonical = await self._apply_mappings(
                    source_system=self.config.name,
                    entity=entity_name,
                    record=record
                )
                await self._emit_canonical_event(canonical)
```

**Connector Configuration (JSON):**

```json
{
  "name": "stripe",
  "type": "generic_rest",
  "api_base_url": "https://api.stripe.com/v1",
  "auth": {
    "type": "bearer_token",
    "secret_key": "STRIPE_SECRET_KEY"
  },
  "rate_limit": "100/second",
  "entities": {
    "customers": {
      "endpoint": "/customers",
      "pagination": {"type": "cursor", "param": "starting_after"},
      "canonical_entity": "customer"
    }
  }
}
```

**Realistic Scaling Impact:**
```
Scenario: 100 new connectors needed

If all were generic (unrealistic):
  100 connectors Ã— 100 config lines = 10,000 lines total
  
Realistic breakdown:
  40 generic connectors Ã— 100 config lines = 4,000 config lines
  40 hybrid connectors Ã— (100 config + 150 code) = 10,000 lines
  20 custom connectors Ã— (100 config + 500 code) = 12,000 lines
  
  Total: 26,000 lines (vs 50,000 if all hand-coded)
  Savings: 48% code reduction, not 90%
```

**Recommendation: Adapter Pattern**
```python
class ConnectorFactory:
    """Choose right connector type based on API characteristics"""
    
    def create_connector(self, config: ConnectorConfig):
        if config.type == "generic_rest":
            return GenericRESTConnector(config)
        elif config.type == "salesforce":
            return SalesforceConnector(config)  # Custom
        elif config.type == "hubspot":
            # Hybrid: Generic base + custom adapters
            base = GenericRESTConnector(config)
            return HubSpotAdapter(base, config)
```

---

### Principle 4: Distributed, Event-Driven Architecture

**Replace synchronous loops with distributed workers:**

**Before (Synchronous):**
```python
for connector in connectors:
    await scan_schema(connector)  # Sequential, slow
```

**After (Distributed):**
```python
# API publishes to Redis Stream
await redis.xadd("schema_scan_jobs", {
    "connector_id": connector.id,
    "tenant_id": tenant.id,
    "priority": "high"
})

# Worker pool processes in parallel (10 workers)
# 1000 connectors Ã· 10 workers = 100 connectors each
# 100 Ã— 5s = 500s = 8 minutes (vs 83 minutes sequential)
```

**Architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway (Rate Limiting, Auth)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis Streams (Job Queue)                   â”‚
â”‚ - schema_scan_jobs                          â”‚
â”‚ - canonical_event_stream                    â”‚
â”‚ - drift_detection_jobs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Pool (Auto-Scaling)                  â”‚
â”‚ - SchemaScanner Workers (10-100)            â”‚
â”‚ - CanonicalPublisher Workers (10-100)       â”‚
â”‚ - DriftDetector Workers (10-100)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL (Multi-Tenant, Partitioned)      â”‚
â”‚ - mapping_registry                          â”‚
â”‚ - canonical_streams (partitioned by tenant) â”‚
â”‚ - drift_events                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Horizontal scaling (add more workers = process more connectors)
- âœ… Fault tolerance (worker crash = job reprocessed)
- âœ… Priority queuing (critical connectors first)
- âœ… Backpressure handling (queue full = API returns 429)

---

### Principle 5: Multi-Tenant Resource Isolation

**Enforce quotas and rate limits per tenant:**

```python
class TenantResourceManager:
    """Enforce quotas per tenant"""
    
    async def check_quota(self, tenant_id: str, resource: str) -> bool:
        quotas = await self._get_tenant_quotas(tenant_id)
        usage = await self._get_current_usage(tenant_id, resource)
        
        if resource == "active_connectors":
            return usage < quotas.max_connectors
        elif resource == "llm_calls_monthly":
            return usage < quotas.max_llm_calls
        elif resource == "canonical_events_hourly":
            return usage < quotas.max_events_per_hour
        
        return True
    
    async def enforce_rate_limit(self, tenant_id: str):
        """Per-tenant rate limiting"""
        key = f"rate_limit:{tenant_id}:api_calls"
        count = await redis.incr(key)
        
        if count == 1:
            await redis.expire(key, 60)  # 1 minute window
        
        limit = await self._get_tenant_rate_limit(tenant_id)
        
        if count > limit:
            raise RateLimitExceeded(f"Tenant {tenant_id} exceeded {limit} calls/min")
```

**PostgreSQL Table Partitioning:**
```sql
-- Partition canonical_streams by tenant_id for isolation
CREATE TABLE canonical_streams (
    id UUID,
    tenant_id VARCHAR NOT NULL,
    entity VARCHAR,
    data JSONB,
    created_at TIMESTAMP
) PARTITION BY HASH (tenant_id);

-- Create 100 partitions for horizontal scaling
CREATE TABLE canonical_streams_p0 PARTITION OF canonical_streams
FOR VALUES WITH (MODULUS 100, REMAINDER 0);

-- ... 99 more partitions
```

**Benefits:**
- âœ… SLA guarantees (each tenant gets reserved capacity)
- âœ… Billing fairness (pay for what you use)
- âœ… Security compliance (data physically isolated)
- âœ… Performance isolation (one tenant can't slow others)

---

### Principle 6: Enterprise Observability

**OpenTelemetry instrumentation:**

```python
from opentelemetry import trace, metrics

tracer = trace.get_tracer(__name__)
meter = metrics.get_meter(__name__)

# Metrics
connector_sync_counter = meter.create_counter(
    "aam.connector.syncs",
    description="Number of connector syncs"
)

mapping_confidence_histogram = meter.create_histogram(
    "aam.mapping.confidence",
    description="Distribution of mapping confidence scores"
)

# Tracing
@tracer.start_as_current_span("sync_connector")
async def sync_connector(connector_id: str, tenant_id: str):
    span = trace.get_current_span()
    span.set_attribute("connector.id", connector_id)
    span.set_attribute("tenant.id", tenant_id)
    
    with tracer.start_span("fetch_data") as fetch_span:
        data = await fetch_from_api(connector_id)
        fetch_span.set_attribute("records.count", len(data))
    
    with tracer.start_span("apply_mappings") as map_span:
        canonical = await apply_mappings(data)
        map_span.set_attribute("mappings.rag", rag_count)
        map_span.set_attribute("mappings.llm", llm_count)
    
    connector_sync_counter.add(1, {"connector": connector_id})
```

**Dashboard Metrics:**
- Connector sync success rate (per tenant, per connector)
- Mapping confidence distribution
- RAG hit rate (target: >95%)
- LLM call volume (target: <5%)
- P50/P95/P99 latencies
- Cost per tenant

---

## Missing Critical Components

**The following production-critical components were NOT addressed in the initial plan:**

### 1. Production Secrets Management & Credential Rotation

**Problem:** Managing API credentials for 100+ connectors in production

**Production Solution:**
```python
# Real Production Runtime API
class ProductionSecretsManager:
    """Centralized secrets management with rotation"""
    
    def __init__(self):
        # Use Vault (HashiCorp) for production
        self.vault_client = self._init_vault()
    
    async def get_connector_credentials(self, connector_id: str, tenant_id: str):
        """Get credentials with automatic rotation detection"""
        secret_path = f"tenants/{tenant_id}/connectors/{connector_id}"
        
        credentials = await self.vault_client.get_secret(secret_path)
        
        # Check if credential rotation needed
        if await self._needs_rotation(credentials):
            credentials = await self._rotate_credentials(connector_id, credentials)
        
        return credentials
    
    async def _rotate_credentials(self, connector_id: str, old_creds: dict):
        """Auto-rotate expiring credentials"""
        # Example: OAuth2 refresh token flow
        new_creds = await self._refresh_oauth_token(old_creds)
        await self.vault_client.update_secret(secret_path, new_creds)
        
        # Log rotation for audit
        await self.audit_log.record_rotation(connector_id, timestamp=now())
        
        return new_creds
```

---

### 2. Audit Trail & Compliance Controls

**Problem:** No GDPR/SOC2 audit trail for data access

**Solution:**
```sql
-- Audit log table
CREATE TABLE audit_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id VARCHAR NOT NULL,
    user_id VARCHAR,
    action VARCHAR NOT NULL,  -- 'mapping_proposed', 'mapping_approved', 'connector_added'
    resource_type VARCHAR NOT NULL,  -- 'mapping', 'connector', 'canonical_event'
    resource_id VARCHAR NOT NULL,
    old_value JSONB,
    new_value JSONB,
    ip_address VARCHAR,
    user_agent VARCHAR,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_audit_tenant_time ON audit_log(tenant_id, created_at DESC);
CREATE INDEX idx_audit_resource ON audit_log(resource_type, resource_id);
```

```python
class AuditLogger:
    """GDPR/SOC2 compliant audit logging"""
    
    async def log_mapping_approval(self, mapping: MappingProposal, user: User):
        await self.db.execute("""
            INSERT INTO audit_log 
            (tenant_id, user_id, action, resource_type, resource_id, new_value, ip_address)
            VALUES ($1, $2, $3, $4, $5, $6, $7)
        """, 
            mapping.tenant_id,
            user.id,
            "mapping_approved",
            "mapping",
            mapping.id,
            mapping.to_json(),
            user.ip_address
        )
    
    async def generate_compliance_report(self, tenant_id: str, start_date, end_date):
        """Generate SOC2 audit report"""
        return await self.db.fetch("""
            SELECT 
                action,
                COUNT(*) as count,
                ARRAY_AGG(DISTINCT user_id) as users
            FROM audit_log
            WHERE tenant_id = $1
              AND created_at BETWEEN $2 AND $3
            GROUP BY action
            ORDER BY count DESC
        """, tenant_id, start_date, end_date)
```

---

### 3. Circuit Breakers & Graceful Degradation

**Problem:** One failing connector can cascade to system-wide failure

**Solution:**
```python
from circuitbreaker import CircuitBreaker, CircuitBreakerError

class ResilientConnector:
    """Connector with circuit breaker and fallback"""
    
    def __init__(self, connector_id: str):
        self.connector_id = connector_id
        # Circuit opens after 5 failures, half-open after 60s
        self.breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60,
            expected_exception=APIError
        )
    
    @CircuitBreaker(failure_threshold=5, recovery_timeout=60)
    async def sync_data(self):
        """Sync with circuit breaker protection"""
        try:
            return await self._fetch_from_api()
        except APIError as e:
            # Circuit breaker tracks failures
            raise
    
    async def sync_with_fallback(self):
        """Graceful degradation on failure"""
        try:
            return await self.sync_data()
        except CircuitBreakerError:
            # Circuit is open - use cached data
            logger.warning(f"Circuit open for {self.connector_id}, using cache")
            return await self._get_cached_data()
        except APIError as e:
            # API error - degrade gracefully
            logger.error(f"API error for {self.connector_id}: {e}")
            await self._notify_ops_team(e)
            return None  # Don't fail entire pipeline
```

**System-Wide Circuit Breaker Dashboard:**
```python
class CircuitBreakerMonitor:
    """Monitor all connector health"""
    
    async def get_system_health(self) -> dict:
        return {
            "total_connectors": 1000,
            "healthy": 950,  # Circuit closed
            "degraded": 30,  # Circuit half-open
            "failed": 20,    # Circuit open
            "overall_health": "95% healthy"
        }
    
    async def should_allow_new_connections(self) -> bool:
        health = await self.get_system_health()
        # Backpressure: Don't accept new connectors if system degraded
        return health["healthy"] / health["total_connectors"] > 0.80
```

---

### 4. Incident Response & Rollback

**Problem:** No automated rollback on production issues

**Solution:**
```python
class IncidentResponseSystem:
    """Automated incident detection and rollback"""
    
    async def monitor_system_health(self):
        """Continuous health monitoring"""
        metrics = await self.get_metrics()
        
        # Detect anomalies
        if metrics.error_rate > 0.05:  # >5% error rate
            await self.trigger_incident("high_error_rate", metrics)
        
        if metrics.p99_latency > 5000:  # >5s latency
            await self.trigger_incident("high_latency", metrics)
        
        if metrics.llm_cost_hourly > 100:  # $100/hour LLM cost
            await self.trigger_incident("cost_spike", metrics)
    
    async def trigger_incident(self, incident_type: str, metrics: dict):
        """Automated incident response"""
        incident_id = await self.create_incident(incident_type, metrics)
        
        # Step 1: Alert on-call team
        await self.pagerduty.alert(incident_id, severity="critical")
        
        # Step 2: Auto-remediation
        if incident_type == "high_error_rate":
            # Rollback recent deployment
            await self.rollback_to_last_good_version()
        
        elif incident_type == "cost_spike":
            # Disable LLM mapping, use RAG-only
            await self.feature_flags.set("use_llm_mapping", False)
        
        # Step 3: Capture diagnostics
        await self.capture_diagnostics(incident_id)
    
    async def rollback_to_last_good_version(self):
        """Database-backed config rollback"""
        # Rollback mapping_registry to previous version
        await self.db.execute("""
            UPDATE mapping_registry
            SET is_active = FALSE
            WHERE version = (SELECT MAX(version) FROM mapping_registry)
        """)
        
        await self.db.execute("""
            UPDATE mapping_registry
            SET is_active = TRUE
            WHERE version = (SELECT MAX(version) - 1 FROM mapping_registry)
        """)
        
        logger.info("Rolled back mapping_registry to previous version")
```

---

### 5. Data Lineage & Governance

**Problem:** Can't trace which connector contributed to canonical event

**Solution:**
```sql
-- Data lineage tracking
CREATE TABLE canonical_event_lineage (
    canonical_event_id UUID REFERENCES canonical_streams(id),
    source_connector_id VARCHAR NOT NULL,
    source_record_id VARCHAR NOT NULL,
    mapping_version INT NOT NULL,
    transformed_at TIMESTAMP DEFAULT NOW(),
    transformation_metadata JSONB,  -- Which fields were mapped how
    
    PRIMARY KEY (canonical_event_id, source_connector_id)
);

-- Query: "Which connectors contributed to this customer record?"
SELECT 
    c.connector_name,
    l.source_record_id,
    l.mapping_version,
    l.transformed_at
FROM canonical_event_lineage l
JOIN connectors c ON c.id = l.source_connector_id
WHERE l.canonical_event_id = '123e4567-e89b-12d3-a456-426614174000';
```

---

## Scalability Comparison Table

| Metric | Initial Plan | Realistic Target | Aspirational Goal | Notes |
|--------|--------------|------------------|-------------------|-------|
| **Onboarding 1000 Connectors** | 27 hours | 2 hours (13x faster) | 30 min (54x faster) | Validated via benchmarking |
| **LLM Call Cost** | $500/cycle | $100/cycle (5x cheaper) | $50/cycle (10x cheaper) | RAG hit rate dependent |
| **Mapping Storage** | 1000 YAML files | PostgreSQL database | PostgreSQL + pgvector | Migration required |
| **Schema Scan Time** | 83 min sequential | 15 min parallel (5x faster) | 8 min (10x faster) | Worker count dependent |
| **Code per Connector** | 500 lines Python (all) | 250 lines avg (40% generic, 60% custom) | 150 lines avg | See capability matrix |
| **RAG Hit Rate** | 0% (no RAG) | 85% after 100 connectors | 95% after training | Quality gates required |
| **Mapping Versioning** | âŒ None (file replace) | âœ… Full history + rollback | âœ… Full history + rollback | Database-backed |
| **Runtime Updates** | âŒ Requires redeploy | âœ… Instant via database | âœ… Instant via database | No code changes |
| **Multi-Tenant Isolation** | âŒ None | âœ… Full isolation + quotas | âœ… Full isolation + quotas | Critical for production |
| **Rate Limiting** | âŒ Global only | âœ… Per-tenant granular | âœ… Per-tenant granular | Redis-based |
| **Observability** | âŒ Basic logs | âœ… OpenTelemetry traces | âœ… Full tracing + metrics | Infrastructure needed |
| **Horizontal Scaling** | âŒ Monolith | âœ… Worker pools (10-50 workers) | âœ… Auto-scaling (10-100+) | Redis Streams required |
| **Fault Tolerance** | âŒ Single point failure | âœ… Circuit breakers + retry | âœ… Full resilience + fallback | See missing components |
| **A/B Testing Mappings** | âŒ Not possible | âœ… Version-based testing | âœ… Multi-version + canary | Database versioning |
| **Secrets Management** | âŒ Hardcoded/env vars | âœ… Integration Manager + rotation | âœ… Vault + auto-rotation | See missing components |
| **Audit Trail** | âŒ None | âœ… Full audit log (GDPR) | âœ… SOC2 compliance | See missing components |

**Key Insight:** Realistic targets are achievable with proper implementation. Aspirational goals require additional optimization and infrastructure investment.

---

## Phased Implementation Plan (Logical Stages & Outcomes)

### Implementation Principles
- **No Patches**: Every change must be fundamentally proper and enterprise-grade
- **No Band-Aids**: Build for scale from the start, not quick fixes
- **Complete Functionality**: Never sacrifice features to pass tests
- **Logical Dependencies**: Each phase enables the next through capability building
- **Measurable Outcomes**: Success criteria based on capabilities achieved

### Phase 0: Foundation Architecture & Core Validation
**Outcome:** Enterprise-grade persistence layer and validated architectural assumptions

**Critical: This phase establishes the fundamental data architecture properly from the start**

**Tasks:**
1. **Benchmark Current System Performance**
   ```python
   # Measure baseline metrics
   - Onboarding time for 10 connectors (currently: ~4.5 hours)
   - LLM cost for 500 fields (currently: ~$5)
   - Mapping lookup latency (currently: YAML file reads)
   - Schema scan time for 10 APIs
   ```

2. **RAG Proof-of-Concept**
   ```python
   # Test RAG with existing 8 YAML mappings
   - Convert 8 YAML files â†’ 200 field mappings
   - Create embeddings (pgvector)
   - Test similarity search with 50 new fields
   - Measure RAG hit rate (target: >75% to proceed)
   ```

3. **Generic Connector Prototype**
   ```python
   # Test with 3 different APIs
   - Stripe (simple REST + cursor pagination)
   - GitHub (bearer auth + OpenAPI spec)
   - Mock CRM (custom pagination)
   
   # Validate: Can 1 connector handle all 3?
   ```

4. **Migration Strategy Validation**
   ```python
   # Test YAML â†’ Database migration with 1 connector
   - Dual-write mode (YAML + DB)
   - Validate output consistency
   - Test rollback (DB â†’ YAML fallback)
   ```

**Success Criteria:**
- Database schema properly normalized with correct foreign keys and indexes
- RAG infrastructure achieves >75% hit rate on test mappings  
- Generic connector pattern successfully handles 3 different API types
- Zero data loss in YAML to database migration path
- Baseline metrics fully documented for progress measurement

**Phase Dependencies:**
- Must complete before any production code changes
- Validates architectural decisions with real data
- Establishes foundation for all subsequent phases

---

### Phase 1: Intelligence Layer & Mapping Infrastructure
**Outcome:** Production-grade mapping system with RAG-first intelligence

**Prerequisites:**
- Phase 0 completed with validated architecture
- PostgreSQL with Pinecone vector store ready
- Redis infrastructure operational

**Tasks:**
1. **Create Database Schema**
   - `mapping_registry` table with versioning
   - `mapping_embeddings` table with pgvector
   - `audit_log` table for compliance
   - Indexes for performance

2. **YAML to Database Migration**
   - Implement dual-write capability (YAML + DB simultaneously)
   - Validate data consistency with zero discrepancies
   - Enable gradual cutover with rollback capability

3. **Build RAG Infrastructure**
   - RAG governance with quality gates
   - Tenant-scoped vector search
   - Cold-start baseline mappings
   - Conflict detection

4. **Implement Secrets Management**
   - Integration Manager for secrets
   - Credential rotation logic
   - Per-tenant isolation

**Success Criteria:**
- All mappings migrated to database with complete data integrity
- RAG infrastructure achieving >85% hit rate on existing connectors
- Sub-100ms mapping lookup latency under load
- Complete audit trail for all mapping operations
- Rollback capability validated with zero data loss

**Phase Dependencies:**
- Enables runtime mapping updates without code changes
- Required for multi-tenant isolation in later phases
- Foundation for all intelligent mapping capabilities

---

### Phase 2: Scalable Connector Architecture
**Outcome:** Adapter pattern supporting both generic and custom connectors

**Prerequisites:**
- Phase 1 intelligence layer fully operational
- Connector capability matrix documented
- Integration Manager handling credentials

**Tasks:**
1. **Build GenericRESTConnector (40% use case)**
   - Standard REST endpoints
   - Bearer token / API key auth
   - Cursor / offset pagination
   - JSON response handling
   - Rate limiting (token bucket)

2. **Build Adapter Pattern Infrastructure**
   - `ConnectorFactory` for type selection
   - `ConnectorCapabilityMatrix` for classification
   - Hybrid adapters (e.g., HubSpotAdapter)

3. **Test with Real APIs**
   - Stripe via Integration Manager (generic)
   - GitHub via Integration Manager (generic with OpenAPI)
   - Mock CRM API (custom pagination - hybrid)

4. **Keep Custom Connectors**
   - Salesforce remains custom (SOQL, Bulk API)
   - MongoDB remains custom (query language)
   - Document what makes them custom

**Success Criteria:**
- GenericRESTConnector successfully handles standard REST APIs
- Custom connectors properly isolated with documented complexity drivers
- Hybrid approach validated for edge cases
- Connector factory correctly routes based on API characteristics
- New connectors deployable through configuration without code changes

**Phase Dependencies:**
- Required for scaling beyond hand-coded connectors
- Enables rapid onboarding of new data sources
- Foundation for distributed processing in next phase

---

### Phase 3: Distributed Processing Infrastructure
**Outcome:** Parallel processing capability for enterprise scale

**Prerequisites:**
- Phase 2 connector framework fully operational
- Redis Streams infrastructure ready
- Worker orchestration platform available

**Tasks:**
1. **Refactor to Event-Driven Architecture**
   - Schema scanner publishes to Redis Stream
   - Canonical publisher consumes from stream
   - Job queue with priority levels

2. **Build Worker Pool**
   - Start with 10 workers (not 100)
   - Test with 50 connectors initially
   - Measure actual throughput

3. **Add Resilience**
   - Circuit breakers per connector
   - Job retry with exponential backoff
   - Graceful degradation (cache fallback)

4. **Table Partitioning (Start Small)**
   - Partition `canonical_streams` by tenant_id (10 partitions, not 100)
   - Test with 3 tenants
   - Validate isolation

**Success Criteria:**
- Event-driven architecture properly decoupled with Redis Streams
- Worker pool successfully processes jobs in parallel
- Circuit breakers prevent cascade failures
- Database partitioning ensures tenant isolation
- System gracefully degrades under load

**Phase Dependencies:**
- Required for handling 100+ concurrent connectors
- Enables horizontal scaling of processing capacity
- Foundation for multi-tenant resource isolation

---

### Phase 4: Enterprise Multi-Tenancy & Observability
**Outcome:** Production-grade resource isolation with comprehensive monitoring and reporting

**Prerequisites:**
- Phase 3 distributed processing operational
- OpenTelemetry infrastructure ready
- Multiple test tenants configured

**Tasks:**
1. **Implement Tenant Resource Management**
   - Per-tenant quotas (connectors, LLM calls, events/hour)
   - Redis-based rate limiting
   - Quota enforcement at API gateway

2. **Add OpenTelemetry Instrumentation**
   - Traces for all connector operations
   - Metrics for RAG hit rate, LLM cost, latency
   - Start simple (basic tracing), expand later

3. **Build Monitoring Dashboard**
   - System health overview
   - Per-tenant usage metrics
   - Cost attribution (LLM spend per tenant)
   - RAG hit rate tracking

4. **Develop Enterprise Reporting Views (Strategic Initiative)**
   - **Tabular Lineage Grid:**
     * Searchable, filterable data grid for all mappings and lineage
     * Column-based views: Source, Table, Field, Target Entity, Mapping Type, Confidence Score
     * Export capabilities (CSV, JSON) for evaluation analysis
     * Real-time filtering by connector, tenant, status, date range
     * Performance metrics per mapping (success rate, processing time)
   - **Hierarchical Tree View:**
     * Collapsible tree structure: Source â†’ Table â†’ Field â†’ Target Mapping
     * Visual ontology representation with drag-and-drop navigation
     * Color-coded confidence levels and mapping status
     * Expandable metadata panels showing transformation rules
     * Cross-reference navigation between source fields and canonical entities

5. **Add Compliance & Audit Components**
   - Audit log for compliance (GDPR)
   - Data lineage tracking with visual flow
   - Incident response automation
   - Test coverage reporting dashboard

**Success Criteria:**
- Per-tenant resource quotas fully enforced
- Rate limiting operational with proper 429 responses
- Complete observability with end-to-end tracing
- Per-tenant cost attribution fully accurate
- Audit trail compliant with GDPR requirements
- Enterprise reporting views enabling rapid evaluation and debugging
- Complete visibility into mapping lineage and data flow

**Phase Dependencies:**
- Required for production SaaS deployment
- Enables fair resource allocation across tenants
- Foundation for compliance and cost management
- Critical for functional testing and evaluation at scale

---

### Phase 5: Production Validation & Scale Testing
**Outcome:** Validated architecture meeting enterprise performance targets

**Prerequisites:**
- All previous phases successfully completed
- Full monitoring infrastructure operational
- Load testing environment configured

**Tasks:**
1. **Baseline Measurement**
   - Capture all metrics from Phase 0 again
   - Compare: Before vs After enterprise architecture
   - Validate improvements (target: 5x-10x)

2. **Progressive Load Testing**
   - Start: 10 connectors (current state)
   - Scale to: 25 connectors
   - Scale to: 50 connectors
   - Scale to: 100 connectors
   - Measure at each step

3. **RAG Hit Rate Validation**
   - Test with diverse connector types
   - Measure RAG hit rate progression
   - Target: 85% at 100 connectors

4. **Multi-Tenant Stress Test**
   - 3 tenants with 25 connectors each (75 total)
   - Validate isolation (no cross-tenant interference)
   - Measure per-tenant latency

5. **Cost Analysis**
   - Calculate actual LLM costs for 100 connectors
   - Validate 5x cost reduction vs initial plan
   - Project costs for 1000 connectors

**Success Criteria:**
- 100 connectors fully operational with measured performance
- RAG achieving >85% hit rate across diverse connector types
- LLM costs reduced by 5x versus initial baseline
- Complete tenant isolation with zero interference
- P95 latency meeting enterprise SLA requirements

**Phase Dependencies:**
- Final validation of entire architecture
- Establishes performance baselines for production
- Confirms readiness for enterprise deployment

**Production Readiness Checklist:**
- Architecture supports 100 connectors with path to 1000
- All enterprise features operational (multi-tenancy, observability, resilience)
- Cost model validated and sustainable
- Performance meets or exceeds targets

### Future Phases (Post-Production)

**Phase 6: Scale Optimization**
- Expand from 100 to 500 to 1000 connectors
- Optimize RAG for 95% hit rate
- Implement auto-scaling infrastructure

**Phase 7: Advanced Capabilities**
- A/B testing for mapping strategies
- Canary deployments for new connectors
- Machine learning optimization of mapping proposals

---

## Enterprise Reporting Views Specification

### Purpose
Enable comprehensive evaluation and functional testing of the AAM-DCL integration at enterprise scale through purpose-built reporting interfaces.

### View 1: Tabular Lineage Grid

**Core Features:**
- **Data Model:**
  ```sql
  - source_system (varchar)
  - source_table (varchar)
  - source_field (varchar)
  - canonical_entity (varchar)
  - canonical_field (varchar)
  - mapping_type (enum: direct, computed, aggregated, transformed)
  - confidence_score (float 0-1)
  - transformation_rule (jsonb)
  - last_validated (timestamp)
  - success_rate (float)
  - avg_processing_time_ms (integer)
  - error_count (integer)
  - tenant_id (varchar)
  ```

- **Search & Filter Capabilities:**
  - Full-text search across all fields
  - Multi-column filtering with AND/OR logic
  - Date range filters for temporal analysis
  - Confidence threshold filtering
  - Status filtering (active, deprecated, failed, pending)

- **Export Formats:**
  - CSV for Excel analysis
  - JSON for programmatic testing
  - SQL for database import

- **Performance Requirements:**
  - <500ms response for 10,000 rows
  - Paginated results (100 rows default)
  - Server-side sorting and filtering

### View 2: Hierarchical Tree View

**Core Features:**
- **Tree Structure:**
  ```
  ðŸ“ Salesforce (Connector)
    â”œâ”€â”€ ðŸ“Š Accounts (Table)
    â”‚   â”œâ”€â”€ ðŸ”¤ Id â†’ canonical.account.id [100% confidence]
    â”‚   â”œâ”€â”€ ðŸ”¤ Name â†’ canonical.account.name [100% confidence]
    â”‚   â””â”€â”€ ðŸ”¤ BillingState â†’ canonical.account.region [85% confidence]
    â”œâ”€â”€ ðŸ“Š Opportunities (Table)
    â”‚   â”œâ”€â”€ ðŸ”¤ Id â†’ canonical.opportunity.id [100% confidence]
    â”‚   â””â”€â”€ ðŸ”¤ Amount â†’ canonical.opportunity.value [95% confidence]
    â””â”€â”€ ðŸ“Š Contacts (Table)
        â””â”€â”€ ðŸ”¤ Email â†’ canonical.contact.email [100% confidence]
  ```

- **Interactive Features:**
  - Expand/collapse at any level
  - Drag to reorganize view
  - Right-click context menu for actions
  - Double-click to view details panel

- **Visual Indicators:**
  - ðŸŸ¢ Green: High confidence (>90%)
  - ðŸŸ¡ Yellow: Medium confidence (70-90%)
  - ðŸ”´ Red: Low confidence (<70%)
  - âš ï¸ Warning: Manual review needed
  - ðŸ”„ Sync: Recently updated

- **Metadata Panel:**
  - Transformation logic viewer
  - Sample data preview
  - Historical changes timeline
  - Related mappings cross-reference

### View 3: Evaluation Dashboard (Additional)

**Key Metrics:**
- **Coverage Analysis:**
  - % of source fields mapped
  - % of canonical entities populated
  - Gaps identification report

- **Quality Metrics:**
  - Average confidence score by connector
  - Error rate trends over time
  - LLM vs RAG usage ratio
  - Processing time distribution

- **Test Suite Integration:**
  - Automated test results overlay
  - Failed mapping highlighting
  - Regression detection alerts
  - A/B testing comparisons

### Technical Implementation

**Backend Requirements:**
- PostgreSQL materialized views for performance
- Redis caching for frequently accessed data
- GraphQL API for flexible querying
- WebSocket for real-time updates

**Frontend Requirements:**
- React-based data grid (AG-Grid or similar)
- D3.js for tree visualization
- Virtual scrolling for large datasets
- Progressive data loading

**API Endpoints:**
```
GET /api/reporting/lineage-grid
  ?page=1&limit=100&sort=confidence_score:desc
  &filter=source_system:salesforce,confidence_score:>0.8

GET /api/reporting/tree-view
  ?connector=salesforce&depth=3&show_metadata=true

GET /api/reporting/evaluation-metrics
  ?tenant_id=xyz&date_from=2024-01-01&granularity=daily

POST /api/reporting/export
  {format: "csv", view: "lineage-grid", filters: {...}}
```

---

## Decision Framework

### Critical Question 1: Demo vs Production?

**Option A: Demonstrate Concepts (Initial Plan)**
- âœ… Fewer phases to complete
- âœ… Shows auto-mapping capabilities
- âŒ Won't scale beyond demo
- âŒ Complete rewrite needed for production
- **Approach:** Focus on visual impact, not scale
- **Risk:** Technical debt, rewrite costs

**Option B: Build for Production (Enterprise Architecture - SELECTED)**
- âœ… Scales to 1000+ connectors
- âœ… Production-ready from day 1
- âœ… No rewrite needed
- âŒ Requires all phases to be properly completed
- **Approach:** Enterprise-grade from the start
- **Risk:** Over-engineering if only demo needed

---

### Critical Question 2: Real API Integrations?

**Real APIs (via Integration Manager):**
- âœ… Demonstrates live data flow
- âœ… More impressive for evaluators
- âŒ Requires API credentials management
- âŒ Rate limiting concerns
- **Best For:** Stripe, GitHub, Slack (well-documented APIs)

**Mock Connectors (CSV/JSON data):**
- âœ… Fast to build (no OAuth setup)
- âœ… Predictable test data
- âœ… No rate limiting
- âŒ Less realistic
- **Best For:** Custom/proprietary systems, rare APIs

**Recommendation:** Mix of 3 real APIs (Stripe, GitHub, Slack via Integration Manager) + 7-10 mock connectors

---

## Implementation Strategy

**Selected Approach:**
- âœ… Option B: Build for Production (Enterprise Architecture)
- âœ… Mix of real APIs + mock connectors for validation
- âœ… Methodology: Correctness First â†’ Reliability â†’ Speed â†’ Scale

### Phased Implementation with Logical Dependencies

**Implementation Philosophy:**
1. **No Patches:** Every change must be fundamentally proper and enterprise-grade
2. **Complete Functionality:** Never sacrifice features to pass tests
3. **Logical Progression:** Each phase enables specific capabilities for the next
4. **Measurable Outcomes:** Success based on capabilities achieved, not timelines

**Phase Dependencies & Outcomes:**
- **Phase 0** â†’ Validates architecture assumptions, establishes proper data layer
- **Phase 1** â†’ Enables intelligent mapping without code changes
- **Phase 2** â†’ Enables rapid connector onboarding via configuration
- **Phase 3** â†’ Enables horizontal scaling for 100+ connectors
- **Phase 4** â†’ Enables production multi-tenant deployment
- **Phase 5** â†’ Validates all capabilities meet enterprise requirements

**Scaling Progression (Capability-Based):**
- Foundation: 10 connectors (validate core architecture)
- Intelligence Layer: 25 connectors (prove RAG effectiveness)
- Generic Framework: 50 connectors (demonstrate configuration-based scaling)
- Distributed Processing: 100 connectors (confirm parallel execution)
- Production Scale: Path to 1000 connectors (future optimization)

---

## Next Steps

**Immediate Actions:**
1. âœ… **Architecture Validated:** Enterprise design reviewed and approved
2. âœ… **Implementation Approach:** Production-ready from the start

**Phase 0 Initiation:**
- Establish proper database architecture with normalized schema
- Validate RAG approach with existing mapping data
- Prototype generic connector pattern
- Confirm migration path maintains data integrity

**Success Gates:**
- Architecture supports enterprise requirements
- RAG achieves target hit rates
- Generic pattern handles standard APIs
- Zero data loss in migrations

**Progressive Capability Building:**
- Each phase adds specific capabilities
- No phase proceeds until dependencies met
- All changes are enterprise-grade
- Never sacrifice functionality for speed

---

## Critical Success Factors

**Architecture Foundation:**
- Database-first design with PostgreSQL as single source of truth
- Proper separation of concerns (AOD discovery, AAM transport, DCL intelligence)
- No external dependencies in critical paths

**Technical Capabilities:**
- RAG achieving >85% hit rate for mapping proposals
- Generic connector pattern covering 40% of use cases
- Event-driven architecture enabling parallel processing
- Complete tenant isolation with zero interference

**Enterprise Requirements:**
- All changes fundamentally proper and scalable
- No patches or band-aids in implementation
- Full functionality maintained (never sacrifice for tests)
- Complete audit trail and compliance features

**Performance Targets:**
- 10x improvement in onboarding speed
- 5x reduction in LLM costs
- Sub-100ms P95 latency for mapping lookups
- Support for 100 concurrent connectors with path to 1000

---

## Key Revisions from Initial Plan

**Performance Targets:**
- Aspirational (initial): 162x faster, 20x cheaper
- Realistic (revised): 10x faster, 5x cheaper
- Stretch goals: 54x faster, 10x cheaper

**Generic Connector Approach:**
- Initial claim: 100% config-driven (unrealistic)
- Revised approach: 40% generic, 60% custom/hybrid (realistic)
- Capability matrix defines what can/cannot be generic

**Scalability:**
- Initial target: 1000 connectors immediately
- Revised target: 100 connectors initially, path to 1000
- Progressive scaling: 10 â†’ 25 â†’ 50 â†’ 100

**Missing Components Added:**
1. Secrets management (Integration Manager + rotation)
2. Audit trail (GDPR/SOC2 compliance)
3. Circuit breakers (graceful degradation)
4. Incident response (automated rollback)
5. Data lineage (governance)

---

**Document Version:** 2.0 (Revised after Enterprise Architecture Review)  
**Date:** November 17, 2025  
**Author:** AutonomOS Architecture Team  
**Review Status:** Validated with high-power model, ready for Phase 0 implementation
