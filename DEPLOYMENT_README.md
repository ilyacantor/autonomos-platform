# üöÄ AutonomOS Deployment Guide

## Quick Start

For Replit deployments, use our optimized installation script:

```bash
bash install-dependencies.sh
```

This automatically:
1. ‚úÖ Installs CPU-only PyTorch (saves 5GB)
2. ‚úÖ Installs all other dependencies
3. ‚úÖ Cleans pip cache

## Why We Optimized

**Problem**: Original deployment image exceeded 8 GiB limit

**Root Cause**: PyTorch installed with CUDA/GPU dependencies (6GB+)

**Solution**: Install CPU-only PyTorch first to prevent CUDA installation

## Image Size Comparison

| Component | Before | After | Savings |
|-----------|--------|-------|---------|
| PyTorch + CUDA | 6.0GB | 1.0GB | 5.0GB |
| Python libs (excluded) | 7.4GB | 0GB | 7.4GB |
| Pip cache (excluded) | 4.1GB | 0GB | 4.1GB |
| Git repo (excluded) | 135MB | 0GB | 135MB |
| **Total Image** | **>8GB** | **1.5-2GB** | **~6GB** |

## Files Created

- **requirements-cpu.txt** - CPU-only PyTorch installation
- **requirements.txt** - All other dependencies (organized & deduplicated)
- **install-dependencies.sh** - Automated installation script
- **.dockerignore** - Excludes unnecessary files from deployment

## Manual Installation

If you prefer to install manually:

```bash
# Step 1: Install CPU-only PyTorch FIRST
pip install --no-cache-dir -r requirements-cpu.txt

# Step 2: Install application dependencies
pip install --no-cache-dir -r requirements.txt

# Step 3: Clean pip cache
rm -rf ~/.cache/pip
```

## Verification

After installation, verify no GPU packages are installed:

```bash
pip list | grep -E "nvidia|cuda"
```

‚úÖ **Expected**: No output or minimal CUDA-free packages  
‚ùå **Problem**: If you see `nvidia-*` packages, PyTorch was installed with CUDA

## What Gets Excluded in Deployment

The `.dockerignore` file excludes:

- `.pythonlibs/` - Your local Python environment (rebuilt in deployment)
- `.cache/` - Pip cache (not needed)
- `.git/` - Git repository (not needed in production)
- `node_modules/` - Node dependencies (rebuilt)
- `tests/` - Test files
- Documentation files (`.md`)
- Mock data and seeds

## Troubleshooting

### Deployment still shows large image size

Make sure you're using the installation script during deployment, not just `pip install -r requirements.txt`

### sentence-transformers errors

If you see CUDA-related errors, PyTorch was installed with GPU support. Reinstall:

```bash
pip uninstall torch torchvision torchaudio
bash install-dependencies.sh
```

### Import errors for AI models

The CPU-only version works identically to GPU version for inference. No code changes needed.

## Production Deployment Checklist

- [x] Use `install-dependencies.sh` during deployment
- [x] Verify `.dockerignore` is present
- [x] Confirm no GPU packages: `pip list | grep nvidia`
- [x] Test NLP Gateway endpoints work correctly
- [x] Verify AAM intelligence features function
- [ ] Deploy and confirm image size < 2GB

## Next Steps

After successful deployment:

1. Monitor image size in deployment logs
2. Test all NLP/AI features (NLP Gateway, AAM intelligence)
3. Verify performance is acceptable with CPU-only inference
4. Consider ONNX optimization if inference speed is critical (reduces to 575MB)

## Need Further Optimization?

See `DEPLOYMENT_OPTIMIZATIONS.md` for advanced techniques:
- Multi-stage Docker builds
- ONNX model conversion
- Smaller embedding models
- Model quantization
