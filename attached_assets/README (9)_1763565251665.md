# AOS Discover v2 - Hybrid Rules + AI/ML Asset Discovery

A comprehensive asset discovery system that combines rule-based logic with AI/ML capabilities for Shadow IT detection and asset classification.

## Features

- **Rule-Based Discovery**: Exact replication of current Discover behaviors (MD5 dedup, priority/confidence scoring, state machine, exception templates)
- **AI/ML Pipeline**: 10 Digital Twin simulators, feature engineering, IsolationForest for Shadow IT detection, LightGBM/GBDT for asset classification
- **Hybrid Mode**: Intelligent correlation of rules + ML outputs
- **Offline & Deterministic**: CPU-only, no outbound calls, seeded for reproducibility
- **FastAPI Service**: RESTful API mirroring CLI functionality

## Quick Start

```bash
# Install dependencies
make setup

# Run full pipeline (gen → featurize → train → validate → discover)
make run

# Or run individual commands
python -m src.cli gen          # Generate 10 digital twins
python -m src.cli featurize    # Build feature table
python -m src.cli train        # Train ML models
python -m src.cli validate     # Validate on holdout twins
python -m src.cli discover     # Run discovery

# Start API server
make api
```

## Architecture

```
aos-discover-v2/
├─ configs/           # Settings and schemas
├─ seeds/             # Drop-in seed data for rules-only mode
├─ archetypes/        # 10 digital twin definitions
├─ data/
│  ├─ raw/           # Simulator output per twin
│  └─ processed/     # features.csv
├─ out/              # Discover results (JSON + markdown)
├─ models/           # Trained ML models
├─ reports/          # Validation metrics and plots
└─ src/
   ├─ cli.py         # Typer CLI
   ├─ app/           # FastAPI service
   ├─ utils/         # IO, RNG, logging, hashing
   ├─ core/          # Schemas, rules, exceptions, correlator
   ├─ sim/           # Digital twin generator + chaos injection
   ├─ signals/       # Auth logs, netflow, observed API/SQL
   ├─ features/      # Feature engineering + graph features
   └─ models/        # IsolationForest + LightGBM/GBDT classifiers
```

## CLI Commands

- `python -m src.cli gen` - Generate 10 digital twins with synthetic signals
- `python -m src.cli featurize` - Build feature table from twin data
- `python -m src.cli train` - Train IsolationForest + Classifier models
- `python -m src.cli validate` - Validate models on holdout twins
- `python -m src.cli discover --mode [rules|ml|hybrid]` - Run asset discovery
- `python -m src.cli all` - Run full pipeline

## API Endpoints

- `GET /healthz` - Health check
- `POST /discover/run` - Run discovery (supports rules/ml/hybrid modes)
- `POST /ml/train` - Train models end-to-end

## Modes

1. **Rules**: Pure rule-based discovery (current behavior)
2. **ML**: Pure ML predictions
3. **Hybrid**: Best of both (recommended)

## Acceptance Criteria

✓ Rules-only parity with current Discover  
✓ 10 digital twins with ≥200 assets total  
✓ ML models: kind F1≥0.70, vendor F1≥0.60, env F1≥0.80  
✓ Shadow IT: precision≥0.60, recall≥0.60  
✓ LightGBM with sklearn GBDT fallback  
✓ API parity with CLI  

## Configuration

Edit `configs/settings.yaml` to adjust:
- Random seed
- Mode (rules/ml/hybrid)
- Feature toggles (graph features, LightGBM)
- Asset counts per twin
- API host/port

## Output

- `out/discover_results.json` - Full discovery results
- `out/discover_report.md` - Human-readable summary
- `reports/validation_metrics.json` - ML metrics
- `reports/confusion_matrix.png` - Classifier performance
- `reports/AI_Validation_Report.md` - Detailed validation report

## Performance

The system has been optimized for production deployment with significant performance improvements:

### Production Performance (Phase 3D - November 2025)

| Metric | Result |
|--------|--------|
| **Reliability** | 0% failures (100% uptime) |
| **Typical Response** | P50: 190ms (78% faster than baseline) |
| **Tail Latency** | P95: 3,900ms (47% faster than baseline) |
| **Throughput** | 22.89 requests/sec (+41% from baseline) |

### Optimized Endpoints

**✅ Fully Compliant (Meet Both P50+P95 SLOs):**
- Dashboard HITL Queues: P50 79ms, P95 280ms
- Dashboard Pipeline: P50 130ms, P95 2,200ms
- NLP Discovery (`/api/discover`): P50 220ms, P95 230ms

### Production Characteristics

- **Concurrent Users:** Optimal for <30 concurrent users
- **Background Services:** Dashboard refresh (30s), HITL audit queue, job orchestration
- **Database:** PostgreSQL with materialized views, indexed queries, atomic updates
- **Caching:** Multi-level (discovery 5-min TTL, dashboard 30s TTL)

### Known Limitations

- Performance degrades under 50+ concurrent users due to database connection pool limits
- Acceptable for typical operational loads (<30 concurrent users)
- See `docs/performance_optimization_summary.md` for complete details

### Load Testing

Run performance tests:
```bash
# Quick smoke test (10 users, 30 seconds)
cd tests/performance
locust -f locustfile.py --users 10 --spawn-rate 2 --run-time 30s --headless --host http://localhost:5000

# Standard load test (50 users, 5 minutes)
locust -f locustfile.py --users 50 --spawn-rate 5 --run-time 5m --headless --host http://localhost:5000
```

See `tests/performance/README.md` for detailed testing documentation.
