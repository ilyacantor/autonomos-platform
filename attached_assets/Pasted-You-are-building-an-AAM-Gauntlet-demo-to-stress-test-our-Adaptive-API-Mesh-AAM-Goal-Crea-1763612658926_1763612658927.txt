You are building an "AAM Gauntlet" demo to stress-test our Adaptive API Mesh (AAM).

Goal:
- Create a small system with:
  1) A Synthetic API Farm service that hosts MANY configurable fake APIs.
  2) An AAM-under-test layer that talks to those APIs using connectors.
  3) A simple web UI that lets us:
     - configure chaos levels,
     - launch standard workflows,
     - visualize how AAM handles auth, rate limits, network errors, schema drift, etc.

Architecture:
- Use Python + FastAPI for the Synthetic API Farm and a minimal AAM runner.
- Use a separate lightweight frontend (React + Vite or Next.js) for the UI.
- Everything can run with docker-compose.

Part 1 – Synthetic API Farm:
- Build a FastAPI app that reads configuration from YAML files in ./api_farm/configs.
- Each YAML defines one synthetic service with fields like:
  - id, protocol (rest only for now), base_path
  - resources and endpoints (method, path)
  - auth.type (oauth2_client_credentials | api_key | none)
  - auth.token_ttl_seconds
  - auth.rotate_token_every_calls
  - rate_limit.max_rps, rate_limit.burst
  - error_profile (e.g. spiky_rate_limit, flaky_5xx, mostly_ok)
  - network_profile (dns_flakiness, tls_flakiness, timeout_probability)
  - schema.version and list of fields
  - drift_schedule: time- or call-based events that mutate the schema
  - tenants with per-tenant overrides.
- On startup, dynamically register routes for every endpoint.
- For each request, enforce:
  - auth rules (issue/validate tokens for oauth2),
  - rate limiting (simple in-memory token bucket per (service, tenant)),
  - error injection based on error_profile and network_profile,
  - current schema version based on drift_schedule.
- Expose a /admin/status endpoint that returns live metrics (per service: calls, 401, 429, 5xx, etc.).

Part 2 – Minimal AAM Layer:
- Implement a Python service (can be same codebase, different module) that:
  - Has a ConnectorInstance object per (service, tenant) with:
    - CredentialDescriptor (auth type, access_token, refresh_token, expires_at, version)
    - RatePolicy (max_rps, burst, backoff policy)
    - NetworkProfile (for now just normal HTTP)
  - Provides functions like:
    - aam_get(connector_id, path, params)
    - aam_post(connector_id, path, json_body)
  - These functions:
    - enforce rate limiting using an in-process token bucket,
    - manage token refresh before expiry,
    - classify errors (auth expired, invalid credentials, rate limit, network, server),
    - implement idempotency for POST (generate and store idempotency keys and results),
    - write failed writes to a small DLQ table (SQLite OK for now).
- Log every call in structured form to a local SQLite DB for the demo:
  - connector_id, tenant_id, timestamp, http_status, error_class, retries, idempotency_key.

Part 3 – Workflows:
- Implement 2–3 simple workflows in Python that use the AAM layer:
  1) “High-volume read” workflow: repeatedly GET from a paginated endpoint.
  2) “Write with idempotency” workflow: POST invoices and handle intermittent 5xx.
  3) “Drift-sensitive” workflow: rely on a field that will be renamed/removed by drift_schedule.
- Provide endpoints to:
  - /demo/run_scenario?mode=mild|storm|hell
    - This will:
      - adjust chaos parameters in the API Farm (e.g. increase error probabilities),
      - run the workflows for N seconds in background tasks.

Part 4 – UI:
- Build a small React frontend that:
  - Calls backend endpoints to:
    - list synthetic services and their live metrics,
    - show AAM metrics (from SQLite logs),
    - trigger /demo/run_scenario with different modes.
  - Show:
    - Per-connector charts of:
      - success vs failure by error_class (auth, rate_limit, network, server),
      - count of token refreshes,
      - count of retries and DLQ entries.
    - A simple topology-like view:
      - left: synthetic APIs (nodes),
      - middle: AAM connector nodes,
      - right: workflows.
      - Use status colors (green/amber/red).

Implementation constraints:
- Keep the code modular and clean: separate api_farm/, aam/, workflows/, ui/.
- Include a docker-compose.yml that brings up:
  - api_farm,
  - aam_backend,
  - frontend.
- Preseed the ./api_farm/configs folder with 5–10 representative services covering:
  - aggressive rate limits,
  - expiring auth,
  - schema drift,
  - network flakiness.
- Include a README explaining:
  - how to run the stack,
  - which scenarios to click in the UI to see AAM’s resilience.

DO NOT over-engineer. This is a demo harness, not production code. Focus on making the chaos visible and the AAM behavior obvious.
