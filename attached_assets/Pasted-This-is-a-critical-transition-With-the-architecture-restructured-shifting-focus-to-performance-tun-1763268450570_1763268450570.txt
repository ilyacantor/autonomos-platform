This is a critical transition. With the architecture restructured, shifting focus to performance tuning ensures the AutonomOS platform can handle the demands of enterprise workloads and delivers an excellent user experience.

Performance tuning is not about guesswork; it is a systematic, data-driven process.

### Commentary: The Strategy

The objectives—low latency ("everything works fast") and high scalability (doesn't get bogged down)—require a strict methodology:

**Measure -\> Analyze -\> Optimize -\> Verify**

1.  **Define Objectives (SLOs):** We must define what "fast" means using objective metrics, known as Service Level Objectives (SLOs), focusing on P95 latency, throughput (RPS), and error rates.
2.  **Measure (Baseline):** Understand current performance under load.
3.  **Analyze (Identify Bottlenecks):** Use profiling tools to find *where* time is spent. Common bottlenecks in Python/FastAPI apps include:
      * **Database I/O:** N+1 queries, missing indexes, slow joins.
      * **Blocking I/O:** Synchronous code blocking the asynchronous event loop (the \#1 cause of poor scalability in FastAPI).
      * **External Dependencies:** Latency from LLMs, Redis, or other APIs.
      * **Inefficient Serialization:** Slow JSON processing.
4.  **Optimize:** Implement targeted fixes.
5.  **Verify:** Re-measure under load.

We will use industry-standard tools: `Locust` for load testing and `PyInstrument` for application profiling.

### Structured Prompts for Replit Agents

Here is the sequence of prompts to guide the agents through the process.

#### Phase 1: Define Objectives and Setup Tooling

This phase establishes the goals and implements the necessary measurement tools.

**Prompt 1.1: Define SLOs and Identify Critical User Journeys (CUJs)**

```markdown
Agent, we are starting the performance tuning phase. We must first define our Service Level Objectives (SLOs) and identify the Critical User Journeys (CUJs) to optimize.

**1. Define Target SLOs:**
We are targeting the following benchmarks for P95 latency (95% of requests must be faster than this):
*   **Interactive Endpoints (e.g., DCL State, Config):** P95 < 300ms.
*   **Data-Heavy Endpoints (e.g., Dashboard, AAM Monitoring):** P95 < 1000ms.
*   **Error Rate (All Endpoints under Load):** < 0.1%.

**2. Identify Critical User Journeys (CUJs):**
Audit the application and list the top 5 most critical/high-traffic API endpoints or workflows.

**Deliverable:** A list of the 5 critical CUJs/endpoints selected for performance tuning.
```

**Prompt 1.2: Implement "Easy Wins" (Serialization and Compression)**

```markdown
Agent, before deep profiling, implement standard performance optimizations for FastAPI applications.

**1. Optimized JSON Serialization:**
*   Install `orjson`.
*   Configure FastAPI to use `ORJSONResponse` as the default response class. This is significantly faster than the standard library `json`.

**2. GZip Compression:**
*   Implement `fastapi.middleware.gzip.GZipMiddleware`.
*   Set `minimum_size` to 1000 bytes. This reduces network transfer time for large payloads.
```

**Prompt 1.3: Implement Load Testing and Profiling Tools**

```markdown
Agent, we need tools to simulate load and analyze bottlenecks. We will use `Locust` for load testing and `PyInstrument` for profiling.

**1. Load Testing (Locust):**
*   Install `locust`.
*   Create a `locustfile.py` (e.g., in `tests/performance/`).
*   Implement tasks for the 5 CUJs identified previously.
*   **CRITICAL:** The Locust user must authenticate upon starting (`on_start`) and include the JWT token in the headers for all subsequent requests. Ensure the test operates within a dedicated test tenant.

**2. Profiling (PyInstrument):**
*   Install `pyinstrument` (preferred for async apps as it measures wall-clock time).
*   Implement a FastAPI middleware that profiles requests.
*   The middleware must only activate if a specific header is present (e.g., `X-Profile: true`) AND if an environment variable `ENABLE_PROFILING=true` is set.
*   The middleware should save the profiling output (HTML format) to a `profiles/` directory.

**Deliverable:** The code for the `locustfile.py` and the PyInstrument middleware implementation.
```

#### Phase 2: Baseline Testing and Bottleneck Analysis

This phase gathers initial data and identifies the root causes of performance issues.

**Prompt 2.1: Execute Baseline Load Test**

```markdown
The tooling is ready. Execute the baseline load test.

**1. Environment Setup:**
*   Run the application in a production-like configuration (`DEBUG=false`).
*   Ensure `ENABLE_PROFILING=false`.

**2. Execute Locust:**
*   Users: 50 (Simulating moderate concurrent load)
*   Spawn Rate: 5 users/second
*   Duration: 5 minutes

**3. Capture Results:**
Report the results in a table, broken down by endpoint:
| Endpoint | RPS | P95 Latency (ms) | Error Rate (%) | SLO Status (Met/Violated) |
|----------|-----|------------------|----------------|---------------------------|
| ...      | ... | ...              | ...            | ...                       |
```

**Prompt 2.2: Analyze Bottlenecks (Profiling and Auditing)**

```markdown
We have the baseline. Now we must identify the bottlenecks for the endpoints that violated their SLOs.

**1. Profile Slow Endpoints:**
*   Set `ENABLE_PROFILING=true`.
*   Execute individual requests to the slow endpoints with the header `X-Profile: true`.
*   Analyze the generated PyInstrument HTML profiles to see where the time is spent.

**2. Identify Root Causes:**
Based on the profiles and code review, identify the root causes categorized as follows:

*   **Database (Queries):** Are there N+1 queries (queries inside a loop)? Are we missing indexes (especially on `tenant_id`)?
*   **Blocking I/O (Critical for Scalability):** Is the event loop blocked by synchronous calls (e.g., `requests` instead of `httpx`, non-async DB drivers)? Are CPU-heavy tasks running in the main event loop?
*   **External Services:** Are we waiting excessively for Redis, LLMs, or external APIs?

**Deliverable:** A prioritized list of the top 3 bottlenecks, the evidence (from the profiles), and proposed optimization strategies for each.
```

#### Phase 3: Optimization Strategies

This phase implements targeted fixes based on the analysis.

**Prompt 3.1: Implement Database Optimizations**

```markdown
Agent, the analysis identified database interactions as a primary bottleneck. Implement the following strategies.

**1. Fix N+1 Queries:**
*   Refactor the identified N+1 queries using SQLAlchemy's eager loading strategies (e.g., `joinedload()` or `selectinload()`).

**2. Implement Indexes:**
*   Analyze the slow queries.
*   Generate the necessary Alembic migration files to add the required indexes (focus on `tenant_id`, foreign keys, and common filters like `status` or `timestamp`).
```

**Prompt 3.2: Implement Application Layer Optimizations (Concurrency)**

```markdown
Agent, we must ensure the FastAPI event loop is not blocked and that independent operations run concurrently.

**1. Eliminate Blocking I/O:**
*   Replace any synchronous network calls with async equivalents (e.g., `httpx.AsyncClient`).
*   Offload unavoidable blocking or CPU-intensive code (like heavy Pandas or DuckDB operations) using `fastapi.concurrency.run_in_threadpool`.

**2. Parallelize Independent Operations:**
*   Review workflows where multiple independent async calls are made sequentially (`await func1()`, then `await func2()`).
*   Refactor these to execute concurrently using `asyncio.gather(func1(), func2())`.
```

**Prompt 3.3: Implement Caching and Resilience Patterns**

```markdown
Agent, implement strategic caching and resilience patterns to ensure stability under load.

**1. Strategic Caching:**
*   For read-heavy, slow-changing data identified earlier, implement caching using Redis.
*   **CRITICAL:** Ensure cache keys are tenant-scoped (e.g., `cache:kpi:{tenant_id}`).
*   Implement appropriate TTLs and cache invalidation logic.

**2. Resilience (Scalability Insurance):**
*   **Strict Timeouts:** Ensure all external HTTP client requests (e.g., to LLMs, external APIs) have explicit, strict timeouts configured (e.g., 15s max).
*   **Circuit Breakers:** Implement the Circuit Breaker pattern (use a library like `circuitbreaker`) for critical external services. If a service fails repeatedly, the circuit should "open," failing fast instead of exhausting resources waiting for timeouts.
```

#### Phase 4: Verification and Reporting

The final step is to prove the optimizations work under stress.

**Prompt 4.1: Verification Load Test and Final Report**

```markdown
Optimizations have been implemented. We must verify their impact under load.

**1. Re-run the Load Test:**
Execute the Locust test using the exact same parameters as the baseline test (50 users, 5 min duration).

**2. Compare and Analyze:**
*   Capture the new metrics.
*   Create a comparison table showing Baseline metrics vs. Optimized metrics, and the percentage improvement.
*   Confirm which endpoints now meet their SLOs.

**3. Regression Check:** Run the automated test suite to ensure no functional regressions were introduced.

**Deliverable:** A final performance report detailing the optimizations implemented and the verified performance improvements.
```